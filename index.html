<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>UvA Deep Learning II Course</title>

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="css/bootstrap.min.css" type="text/css">

    <!-- Custom Fonts -->
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Merriweather:400,300,300italic,400italic,700,700italic,900,900italic' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="font-awesome/css/font-awesome.min.css" type="text/css">

    <!-- Plugin CSS -->
    <link rel="stylesheet" href="css/animate.min.css" type="text/css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="css/creative.css" type="text/css">

    <link rel="apple-touch-icon" sizes="57x57" href="icons/apple-icon-57x57.png">
    <link rel="apple-touch-icon" sizes="60x60" href="icons/apple-icon-60x60.png">
    <link rel="apple-touch-icon" sizes="72x72" href="icons/apple-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="76x76" href="icons/apple-icon-76x76.png">
    <link rel="apple-touch-icon" sizes="114x114" href="icons/apple-icon-114x114.png">
    <link rel="apple-touch-icon" sizes="120x120" href="icons/apple-icon-120x120.png">
    <link rel="apple-touch-icon" sizes="144x144" href="icons/apple-icon-144x144.png">
    <link rel="apple-touch-icon" sizes="152x152" href="icons/apple-icon-152x152.png">
    <link rel="apple-touch-icon" sizes="180x180" href="icons/apple-icon-180x180.png">
    <link rel="icon" type="image/png" sizes="192x192"  href="icons/android-icon-192x192.png">
    <link rel="icon" type="image/png" sizes="32x32" href="icons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="96x96" href="icons/favicon-96x96.png">
    <link rel="icon" type="image/png" sizes="16x16" href="icons/favicon-16x16.png">
    <link rel="manifest" href="/manifest.json">
    <meta name="msapplication-TileColor" content="#ffffff">
    <meta name="msapplication-TileImage" content="icons/ms-icon-144x144.png">
    <meta name="theme-color" content="#ffffff">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-86672135-1', 'auto');
  ga('send', 'pageview');

</script>

<body id="page-top">

    <nav id="mainNav" class="navbar navbar-default navbar-fixed-top">
        <div class="container-fluid">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand page-scroll" href="#page-top">UvA Deep Learning II Course</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a class="page-scroll" href="#about">About</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#lectures">Modules</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#practicals">Tutorials</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#course-links">Links</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#contact">Contact</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container-fluid -->
    </nav>

    <header>
        <div class="header-content">
            <div class="header-content-inner">
                <h1>UVA Deep Learning II Course</h1>
                <hr>
                <p>MSc in Artificial Intelligence for the University of Amsterdam.</p>
                <a href="#about" class="btn btn-primary btn-xl page-scroll">Find Out More</a>
            </div>
        </div>
    </header>

    <section class="bg-primary" id="about">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 text-center">
                    <h2 class="section-heading">About</h2>
                    <hr class="light">
                    <p class="text-faded" style="text-align:justify">Deep learning II is taught in the MSc program in Artificial Intelligence of the University of Amsterdam. In this course we study the theory of deep learning, namely of modern, multi-layered neural networks trained on big data. The course is coordinated by Assistant Professors <a href="mailto: e.gavves@uva.nl" style="color: white;">Efstratios Gavves and Wilker Aziz Fereira</a>.
                        
                        <br></br>
                        <a href="mailto: e.gavves@uva.nl""><img class="img-circle" src="images/people/efstratios-gavves.png" hspace="5" width="120" title="Efstratios Gavves"></a>
                            <a href="mailto: e.gavves@uva.nl""><img class="img-circle" src="images/people/wilker-aziz-fereira.png" hspace="5" width="120" title="Wilker-Aziz-Fereira"></a>
                        <br></br>
                            And the lectures for the course are: Erik Bekkers, Eric Nalisnick, Eric Marcus, Sara Magliacane, Andrew Yates, Wilker Aziz Ferreira and Stratis Gaves. 
                            <br></br>
    
                            <a href="mailto: e.j.bekkers@uva.nl""><img class="img-circle" src="images/people/erik.jpg" hspace="5" width="120" title="Erik Bekkers"></a>
                            <a href="mailto: e.t.nalisnick@uva.nl""><img class="img-circle" src="images/people/eric.jpg" hspace="5" width="120" title="Eric Nalisnick"></a>
                            <a href="mailto: e.j.marcus@uva.nl""><img class="img-circle" src="images/people/eric_m.jpg" hspace="5" width="120" title="Eric Marcus"></a>
                            <a href="mailto: s.magliacane@uva.nl""><img class="img-circle" src="images/people/sara.jpg" hspace="5" width="120" title="Sara Magliacane"></a>
                            <a href="mailto: a.c.yates@uva.nl""><img class="img-circle" src="images/people/andrew.jpg" hspace="5" width="120" title="Andrew Yates"></a>

                        <br></br> 
                    The Teaching Assistants (TAs) are:
                    <a href='mailto:c.athanasiadis@uva.nl' target='_blank' style='color: white;'>Christos Athanasiadis</a>, <a href='mailto:i.a.auzina@uva.nl' target='_blank' style='color: white;'>Ilze Amanda Auzina</a>, <a href='mailto:l.f.bereska@uva.nl' target='_blank' style='color: white;'>Leonard Bereska</a>, <a href='mailto:g.cesa@uva.nl' target='_blank' style='color: white;'>Gabriele Cesa</a>, <a href='mailto:e.egorov@uva.nl' target='_blank' style='color: white;'>Evgenii Egorov</a>, <a href='mailto:b.eikema@uva.nl' target='_blank' style='color: white;'>Bryan Eikema</a>, <a href='mailto:r.d.hesselink@uva.nl' target='_blank' style='color: white;'>Rob Hesselink</a>, <a href='mailto:s.h.hsu@uva.nl' target='_blank' style='color: white;'>Cyril Hsu</a>, <a href='mailto:d.m.knigge@uva.nl' target='_blank' style='color: white;'>David Knigge</a>, <a href='mailto:m.kofinas@uva.nl' target='_blank' style='color: white;'>Miltos Kofinas</a>, <a href='mailto:p.a.vanderlinden@uva.nl' target='_blank' style='color: white;'>Putri Linden</a>, <a href='mailto:y.liu6@uva.nl' target='_blank' style='color: white;'>Yongtuo Liu</a>, <a href='mailto:t.nguyen2@uva.nl' target='_blank' style='color: white;'>Thong Nguyen</a>, <a href='mailto:f.p.nolte@uva.nl' target='_blank' style='color: white;'>Frederik Nolte</a>, <a href='mailto:s.papa@uva.nl' target='_blank' style='color: white;'>Samuel Papa</a>, <a href='mailto:a.a.pervez@uva.nl' target='_blank' style='color: white;'>Adeel Pervez</a>, <a href='mailto:sharvaree.vadgama@gmail.com' target='_blank' style='color: white;'>Sharvaree Vadgama</a>, <a href='mailto:r.valperga@uva.nl' target='_blank' style='color: white;'>Riccardo Valperga</a>, <a href='mailto:h.wang3@uva.nl' target='_blank' style='color: white;'>Haochen Wang</a>
                </p>
                   

                    <br></br>
                    <a href='mailto:c.athanasiadis@uva.nl'><img class='img-circle' src='images/people/christos-athanasiadis.jpg' hspace='5' width='120' alt='Christos Athanasiadis' title='Christos Athanasiadis'></a>
<a href='mailto:i.a.auzina@uva.nl'><img class='img-circle' src='images/people/ilze-amanda-auzina.jpg' hspace='5' width='120' alt='Ilze Amanda Auzina' title='Ilze Amanda Auzina'></a>
<a href='mailto:l.f.bereska@uva.nl'><img class='img-circle' src='images/people/leonard-bereska.jpg' hspace='5' width='120' alt='Leonard Bereska' title='Leonard Bereska'></a>
<a href='mailto:g.cesa@uva.nl'><img class='img-circle' src='images/people/gabriele-cesa.jpg' hspace='5' width='120' alt='Gabriele Cesa' title='Gabriele Cesa'></a></br></br>
<a href='mailto:e.egorov@uva.nl'><img class='img-circle' src='images/people/evgenii-egorov.jpg' hspace='5' width='120' alt='Evgenii Egorov' title='Evgenii Egorov'></a>
<a href='mailto:b.eikema@uva.nl'><img class='img-circle' src='images/people/bryan-eikema.jpg' hspace='5' width='120' alt='Bryan Eikema' title='Bryan Eikema'></a>
<a href='mailto:r.d.hesselink@uva.nl'><img class='img-circle' src='images/people/rob-hesselink.jpg' hspace='5' width='120' alt='Rob Hesselink' title='Rob Hesselink'></a>
<a href='mailto:s.h.hsu@uva.nl'><img class='img-circle' src='images/people/cyril-hsu.jpg' hspace='5' width='120' alt='Cyril Hsu' title='Cyril Hsu'></a>
<a href='mailto:d.m.knigge@uva.nl'><img class='img-circle' src='images/people/david-knigge.jpg' hspace='5' width='120' alt='David Knigge' title='David Knigge'></a></br></br>
<a href='mailto:m.kofinas@uva.nl'><img class='img-circle' src='images/people/miltos-kofinas.jpg' hspace='5' width='120' alt='Miltos Kofinas' title='Miltos Kofinas'></a>
<a href='mailto:p.a.vanderlinden@uva.nl'><img class='img-circle' src='images/people/putri-linden.jpg' hspace='5' width='120' alt='Putri Linden' title='Putri Linden'></a>
<a href='mailto:y.liu6@uva.nl'><img class='img-circle' src='images/people/yongtuo-liu.jpg' hspace='5' width='120' alt='Yongtuo Liu' title='Yongtuo Liu'></a>
<a href='mailto:t.nguyen2@uva.nl'><img class='img-circle' src='images/people/thong-nguyen.jpg' hspace='5' width='120' alt='Thong Nguyen' title='Thong Nguyen'></a>
<a href='mailto:f.p.nolte@uva.nl'><img class='img-circle' src='images/people/frederik-nolte.jpg' hspace='5' width='120' alt='Frederik Nolte' title='Frederik Nolte'></a></br></br>
<a href='mailto:s.papa@uva.nl'><img class='img-circle' src='images/people/samuel-papa.jpg' hspace='5' width='120' alt='Samuel Papa' title='Samuel Papa'></a>
<a href='mailto:a.a.pervez@uva.nl'><img class='img-circle' src='images/people/adeel-pervez.jpg' hspace='5' width='120' alt='Adeel Pervez' title='Adeel Pervez'></a>
<a href='mailto:sharvaree.vadgama@gmail.com'><img class='img-circle' src='images/people/sharvaree-vadgama.jpg' hspace='5' width='120' alt='Sharvaree Vadgama' title='Sharvaree Vadgama'></a>
<a href='mailto:r.valperga@uva.nl'><img class='img-circle' src='images/people/riccardo-valperga.jpg' hspace='5' width='120' alt='Riccardo Valperga' title='Riccardo Valperga'></a>
<a href='mailto:h.wang3@uva.nl'><img class='img-circle' src='images/people/haochen-wang.jpg' hspace='5' width='120' alt='Haochen Wang' title='Haochen Wang'></a></br></br>
                </div>
            </div>
        </div>
    </section>

    <section id="lectures">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Modules</h2>
                    <hr class="primary">
                </div>
            </div>

            <div class="lecture-box" style="margin-bottom: 40px;">
                <button type="button" class="lecture-collapsible"><span class="fa fa-2x fa-university text-primary"></span><span class="lecture-title">Module 1: Geometric Deep Learning (Week 2-4)</span></button>
                <div class="lecture-content">
                    <div class="padding-div">
                        <p class="text-muted" style="font-size: 12px"><b>Erik Bekkers </b></p>
                        <p><b>Credits: 2pts</b> <br></br> 100% of the materials for this course are available. <br></br> This module covers the topic of geometric deep learning, touching upon all its five G's (Grids, Groups, Graphs, Geodesics, and Gauges) but with a strong focus on group equivariant deep learning. The impact that CNNs made in fields such as computer vision, computational chemistry and physics, can largely be attributed to the fact that convolutions allow for weight sharing, geometric stability, and a dramatic decrease in learnable parameters by leveraging symmetries in data and architecture design. These enabling properties arise from the equivariance property of convolutions. That is, if the input image is translated, the output of a convolution is translated accordingly, which in turn means that local information does not get lost in the neural network upon an input transformation (it is just shifted to a different location). With group equivariant deep learning we can hard-code stability and weight sharing over transformations beyond just translations. E.g., it allows for sharing of weights (representing complex patterns/representations) over poses and symmetries represented by transformations such as translation + rotation + scaling. This module is split into 4 lectures: <ul><li> <b>Lecture 1 (Wednesday 6th of April on-site Q&A)</b>: Regular group convolutional neural networks (G-CNNs). In this lecture we cover the basics of group convolutional NNs and show how to leverage symmetries in data and practical problems.</li>  <li> <b>Lecture 2 (Tuesday 12th of April on-site Q&A)</b>: Steerable G-CNNs. In this lecture we introduce a very general class of G-CNNs that allows to handle (rotational) symmetries in a flexible and powerful way. These methods are at the core of the most successful methods to handle 3D data such as atomic point clouds, but are also at the core of gauge equivariant methods that are applicable to arbitrary Riemannian manifolds (e.g. 2D shapes embedded in 3D space).</li> <li> <b>Lecture 3 (Tuesday 19th of April on-site Q&A)</b>: Equivariant graph NNs. Many problems in computational chemistry and computational physics are now-a-days solved via graph NNs. The SotA in these domains derive their effectives from the geometric structure and symmetries presented by the data and underlying physics. In this lecture we go over some of the state-of-the-art in geometric graph NNs, discussing several instances of the equivariant message passing paradigm. </li> <li> <b>Lecture 4 (Tuesday 26th of April on-site Q&A)</b>: Geometric latent space models. In this last lecture we take a step back from the group equivariance paradigm and look at what other types of geometric structure can be leveraged in neural networks. We will discuss some models that are based on non-Euclidean latent spaces and go over the required geometric tools to handle them.</li> </ul></p>
                        <h4>Documents:</h4>
                        <div class="lecture-document-list">
                            <a href='lectures/Geometric deep learning/Lecture_1_Complete.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Lecture 1: Group equivariant DL (regular g-convs).</span></a></br>
            <a href='lectures/Geometric deep learning/Lecture_1_1_Motivation.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>1.1) Introduction.</span></a></br>
            <a href='lectures/Geometric deep learning/Lecture_1_2_GroupTheory.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>1.2) Group theory | The basics.</span></a></br>
            <a href='lectures/Geometric deep learning/Lecture_1_3_RegularGroupConvolutions.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>1.3) Regular group convolutions | Template matching viewpoint.</span></a></br>
            <a href='lectures/Geometric deep learning/Lecture_1_4_Example.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>1.4) Equivariant NN Example | With histopathology images.</span></a></br>
            <a href='lectures/Geometric deep learning/Lecture_1_5_History.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>1.5) A brief history of G-CNNs.</span></a></br>
            <a href='lectures/Geometric deep learning/Lecture_1_6_GroupTheory.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>1.6) Group Theory, Homogeneous/quotient spaces.</span></a></br>
            <a href='lectures/Geometric deep learning/Lecture_1_7_GConvsAreAllYouNeed.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>1.7) Group convolutions are all you need! <br></br></span></a></br>
            <a href='lectures/Geometric deep learning/Lecture_2_1.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>2.1) - Steerable kernels/basis functions.</span></a></br>
            <a href='lectures/Geometric deep learning/Lecture_2_2.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>2.2) Revisiting regular G-convs with steerable kernels.</span></a></br>
            <a href='lectures/Geometric deep learning/Lecture_2_3.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>2.3) Group Theory | Irreducible representations and Fourier transformation.</span></a></br>
            <a href='lectures/Geometric deep learning/Lecture_2_4.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>2.4) Induced representations and feature fields</span></a></br>
            <a href='lectures/Geometric deep learning/Lecture_2_5.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>2.5) Steerable group convolutions.</span></a></br>
            <a href='lectures/Geometric deep learning/Lecture_2_6.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>2.6) Activation functions for steerable G-CNNs.</span></a></br>
            <a href='lectures/Geometric deep learning/Lecture_2_7.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>2.7) Derivation of Harmonic nets from regular g-convs.</span></a></br>
            <a href='lectures/Geometric deep learning/Lecture_2_Complete.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>2) Complete slides <br></br></span></a></br>
            <a href='lectures/Geometric deep learning/Lecture_3_1.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>3.1) Motivation for SE(3) equivariant graph NNs.</span></a></br>
            <a href='lectures/Geometric deep learning/Lecture_3_2.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>3.2) Equivariant message passing as non-linear convolution.</span></a></br>
            <a href='lectures/Geometric deep learning/Lecture_3_3.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>3.3) Tensor products as conditional linear layers.</span></a></br>
            <a href='lectures/Geometric deep learning/Lecture_3_4.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>3.4) Group Theory (SO(3) irreps, Wigner-D, Clebsch-Gordan).</span></a></br>
            <a href='lectures/Geometric deep learning/Lecture_3_5.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>3.5) 3D Steerable (graph) convolutions.</span></a></br>
            <a href='lectures/Geometric deep learning/Lecture_3_6.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>3.6) Regular (as opposed to steerable) equivariant graph NNs.</span></a></br>
            <a href='lectures/Geometric deep learning/Lecture_3_7.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>3.7) Gauge equivariant graph NNs.</span></a></br>
                        </div>
                        <h4>Lecture recordings:</h4>
                        <div class="lecture-document-list">
                            <a href='https://www.youtube.com/watch?v=z2OEyUgSH2c&list=PLJ2Aod97Uj8IH7sT4NpM2MOpPeq0H2_lM&index=1'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Lecture 1: Group Equivariant Deep Learning - Lecture 1.1: Introduction.</span></a></br>
            <a href='https://youtu.be/F0OxOCZwm1Q?list=PLJ2Aod97Uj8IH7sT4NpM2MOpPeq0H2_lM'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Lecture 1: Group Equivariant Deep Learning - Lecture 1.2: Group theory (product, inverse, representations).</span></a></br>
            <a href='https://youtu.be/cWG_1IzI0uI?list=PLJ2Aod97Uj8IH7sT4NpM2MOpPeq0H2_lM'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Lecture 1: Group Equivariant Deep Learning - Lecture 1.3: Regular group convolutional neural networks.</span></a></br>
            <a href='https://youtu.be/X3gP1voalDE?list=PLJ2Aod97Uj8IH7sT4NpM2MOpPeq0H2_lM'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Lecture 1: Group Equivariant Deep Learning - Lecture 1.4: Example.</span></a></br>
            <a href='https://youtu.be/kTvow5-eCCQ?list=PLJ2Aod97Uj8IH7sT4NpM2MOpPeq0H2_lM'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Lecture 1: Group Equivariant Deep Learning - Lecture 1.5: A Brief History of G-CNNs.</span></a></br>
            <a href='https://youtu.be/mntjPJYxwTI?list=PLJ2Aod97Uj8IH7sT4NpM2MOpPeq0H2_lM'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Lecture 1: Group Equivariant Deep Learning - Lecture 1.6: Group theory (Homogeneous/quotient spaces).</span></a></br>
            <a href='https://youtu.be/erlCaoj6sTg?list=PLJ2Aod97Uj8IH7sT4NpM2MOpPeq0H2_lM'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Lecture 1: Group Equivariant Deep Learning - Lecture 1.7: Group convolutions are all you need <br></br>.</span></a></br>
            <a href='https://youtu.be/Lm5vZRtGysc?list=PL8FnQMH2k7jzPrxqdYufoiYVHim8PyZWd'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Lecture 2.1: Steerable kernels/basis functions.</span></a></br>
            <a href='https://youtu.be/OHFwsEglmrE?list=PL8FnQMH2k7jzPrxqdYufoiYVHim8PyZWd'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Lecture 2.2: Revisiting Regular G-Convs with Steerable Kernels.</span></a></br>
            <a href='https://youtu.be/qnqcLumE3vg?list=PL8FnQMH2k7jzPrxqdYufoiYVHim8PyZWd'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'> Lecture 2.3: Group Theory (Irreducible representations, Fourier).</span></a></br>
            <a href='https://youtu.be/lkXO0Zi65Uc?list=PL8FnQMH2k7jzPrxqdYufoiYVHim8PyZWd'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Lecture 2.4: Group Theory (Induced representation, feature fields).</span></a></br>
            <a href='https://youtu.be/sDpdtKHgDbE?list=PL8FnQMH2k7jzPrxqdYufoiYVHim8PyZWd'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'> Lecture 2.5: Steerable group convolutions.</span></a></br>
            <a href='https://youtu.be/b8K6adf_zY0?list=PL8FnQMH2k7jzPrxqdYufoiYVHim8PyZWd'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Lecture 2.6: Activation Functions for Steerable G-CNNs.</span></a></br>
            <a href='https://youtu.be/EBzqL1OXigM?list=PL8FnQMH2k7jzPrxqdYufoiYVHim8PyZWd'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Lecture 2.7: Derivation of Harmonic Networks from Regular G-Convs <br></br>.</span></a></br>
            <a href='https://youtu.be/kly4xvYbb8o?list=PLJ2Aod97Uj8Lu1ILU_wX2ohlhfOp6UEBp'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>3.1) Motivation for SE(3) equivariant graph NNs.</span></a></br>
            <a href='https://youtu.be/o-KcYASwUco?list=PLJ2Aod97Uj8Lu1ILU_wX2ohlhfOp6UEBp'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>3.2) Equivariant message passing as non-linear convolution.</span></a></br>
            <a href='https://youtu.be/04hNKKyj_V4?list=PLJ2Aod97Uj8Lu1ILU_wX2ohlhfOp6UEBp'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>3.3) Tensor products as conditional linear layers.</span></a></br>
            <a href='https://youtu.be/cB4zsnLnZfo?list=PLJ2Aod97Uj8Lu1ILU_wX2ohlhfOp6UEBp'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>3.4) Group Theory (SO(3) irreps, Wigner-D, Clebsch-Gordan).</span></a></br>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="lecture-box" style="">
                <button type="button" class="lecture-collapsible"><span class="fa fa-2x fa-university text-primary"></span><span class="lecture-title">Module 2: Bayesian Deep Learning (Week 2)</span></button>
                <div class="lecture-content">
                    <div class="padding-div">
                        <p class="text-muted" style="font-size: 12px"><b> Eric Nalisnick </b></p>
                        <p><b>Credits: 1pts </b><br></br> Usually we train neural networks (NNs) by way of point estimation.  After running gradient descent, each weight is represented by a particular value.  In this module, we consider giving NNs a Bayesian treatment.  We start by specifying a probability distribution for each weight---p(W)---and the goal of training is to obtain the posterior distribution p(W | D), where D denotes the training data, via Bayes rule.  The Bayesian approach provides natural mechanisms for model selection, complexity control, incorporating prior knowledge, uncertainty quantification, and online learning.  Yet, Bayesian inference is computationally demanding, and much of the module will focus on scalable methods for estimating the posterior distribution.</p>
                        <h4>Documents:</h4>
                        <div class="lecture-document-list">
                            <a href='lectures/Bayesian Deep Learning/DL2_BNN_module.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Slides for the module.</span></a></br>
                        </div>
                        <h4>Lecture recordings:</h4>
                        <div class="lecture-document-list">
                            <a href='https://youtu.be/ij2FVxLQuEI?list=PLJ2Aod97Uj8IQKziu0BK2s83cPN3_OHyr'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Bayesian NNs: Motivation and Model Definition.</span></a></br>
            <a href='https://youtu.be/QMLf_McAKWY?list=PLJ2Aod97Uj8IQKziu0BK2s83cPN3_OHyr'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Bayesian NNs: Priors.</span></a></br>
            <a href='https://youtu.be/nm4kx21PuyI?list=PLJ2Aod97Uj8IQKziu0BK2s83cPN3_OHyr'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Bayesian NNs: Posterior Inference.</span></a></br>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="lecture-box" style="margin-bottom: 40px;">
                <button type="button" class="lecture-collapsible"><span class="fa fa-2x fa-university text-primary"></span><span class="lecture-title">Module 3: Deep probabilistic models I (Week 1)</span></button>
                <div class="lecture-content">
                    <div class="padding-div">
                        <p class="text-muted" style="font-size: 12px"><b>Wilker Aziz Ferreira</b></p>
                        <p><b>Credits: 1pts</b><br></br> Add your questions for the Q&A: https://www.dory.app/c/f97ceccf/a8a8baab_dl2---probdl1 <br></br> 100% of the materials for this module are published. <br></br>In this module you learn to view data as a byproduct of probabilistic experiments. You will parameterise joint probability distributions over observed random variables and perform parameter estimation by regularised gradient-based maximum likelihood estimation. <br></br> The Q&A session for this module will take place at 6th of April.</p>
                        <h4>Documents:</h4>
                        <div class="lecture-document-list">
                            <a href='https://canvas.uva.nl/courses/28689/files/folder/lectures/Deep%20probabilistic%20models%20I?preview=6488166'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>NN parameterisation of joint distributions over observed variables</span></a></br>
                        </div>
                        <h4>Lecture recordings:</h4>
                        <div class="lecture-document-list">
                            <a href='https://webcolleges.uva.nl/Mediasite/MyMediasite/presentations/57053ad80d2847888ef9aefcba2574a61d'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Deep probabilistic models I - Lecture 1.1.</span></a></br>
            <a href='https://webcolleges.uva.nl/Mediasite/MyMediasite/presentations/a30fffc800524283809bd69b27a11a351d'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Deep probabilistic models I - Lecture 1.2.</span></a></br>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="lecture-box" style="">
                <button type="button" class="lecture-collapsible"><span class="fa fa-2x fa-university text-primary"></span><span class="lecture-title">Module 4: Deep probabilistic models II (Week 2-3)</span></button>
                <div class="lecture-content">
                    <div class="padding-div">
                        <p class="text-muted" style="font-size: 12px"><b>Wilker Aziz Ferreira</b></p>
                        <p><b>Credits: 2pts</b> <br></br> In this session you learn to model complex data along with unobserved discrete random variables. Examples you are probably already familiar with include mixture models, factor models, and HMMs. You will learn how to approximate intractable inference using stochastic variational methods and derive efficient gradient estimators for parameter estimation via backpropagation.</p>
                        <h4>Documents:</h4>
                        <div class="lecture-document-list">
                            <a href='https://canvas.uva.nl/courses/28689/pages/fundamentals-of-variational-inference?module_item_id=1243071'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Fundamentals of variational inference.</span></a></br>
            <a href='https://canvas.uva.nl/courses/28689/pages/variational-inference-for-deep-discrete-latent-variable-models?module_item_id=1243067'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Variational inference for deep discrete latent variable models. Variational inference for deep discrete latent variable models.</span></a></br>
            <a href='https://canvas.uva.nl/courses/28689/pages/variational-inference-for-deep-continuous-latent-variable-models?module_item_id=1243068'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Variational inference for deep continuous latent variable models (e.g., VAEs), reparameterised gradients beyond Gaussian (e.g., path derivatives, ADVI, implicit reparameterisation).</span></a></br>
            <a href='https://canvas.uva.nl/courses/28689/files/folder/lectures/Deep%20probabilistic%20models%20I?preview=6488165'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Slides for Deep probabilistic models II</span></a></br>
            <a href='https://canvas.uva.nl/courses/28689/files/folder/lectures/Deep%20probabilistic%20models%20II?preview=6565085'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Lecture 2: Deep Continuous Latent Variable Models.</span></a></br>
            <a href='https://canvas.uva.nl/courses/28689/files/folder/lectures/Deep%20probabilistic%20models%20II?preview=6565084'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Lecture 2: Automatic Differentiation Variational Inference.</span></a></br>
                        </div>
                        <h4>Lecture recordings:</h4>
                        <div class="lecture-document-list">
                            <a href='https://webcolleges.uva.nl/Mediasite/MyMediasite/presentations/aa13f3fd4c064f29b0fcb4e43c05c0cc1d'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Lecture 1: Deep probabilistic models II - Lecture 1.3.</span></a></br>
            <a href='https://webcolleges.uva.nl/Mediasite/MyMediasite/presentations/1846886c57834477adc2eb403b64b3481d'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Lecture 1: Deep probabilistic models II - Lecture 1.4.</span></a></br>
            <a href='https://webcolleges.uva.nl/Mediasite/MyMediasite/presentations/6000b98c8a1944c5979cc0ae11690bd61d'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Lecture 1: Deep probabilistic models II - Lecture 1.5.</span></a></br>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="lecture-box" style="margin-bottom: 40px;">
                <button type="button" class="lecture-collapsible"><span class="fa fa-2x fa-university text-primary"></span><span class="lecture-title">Module 5: Advanced approximate inference for discrete latent variable models (Week 4)</span></button>
                <div class="lecture-content">
                    <div class="padding-div">
                        <p class="text-muted" style="font-size: 12px"><b>Wilker Aziz Ferreira</b></p>
                        <p><b>Credits: 1pts </b><br></br> In this session you will learn about techniques to reduce variance in gradient estimation for models with discrete unobserved random variables. We will look into: control variates, Rao-Blackwellisation, continuous relaxations and proxy gradients, sparse projections and mixed random variables.</p>
                        <h4>Documents:</h4>
                        <div class="lecture-document-list">
                            <a href='https://canvas.uva.nl/courses/28689/pages/variance-reduction-proxy-gradients-and-mixed-random-variables?module_item_id=1243069'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Variance reduction (e.g., control variates, Rao-Blackwell).</span></a></br>
            <a href='https://canvas.uva.nl/courses/28689/pages/variance-reduction-proxy-gradients-and-mixed-random-variables'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Proxy gradients (e.g., continuous relaxations, sparse parameterisation of inference models).</span></a></br>
                        </div>
                        <h4>Lecture recordings:</h4>
                        <div class="lecture-document-list">
                            <a href='https://surfdrive.surf.nl/files/index.php/s/1Bq2ImpGElGwePL#/files_mediaviewer/Module5.mp4'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Module 5 record.</span></a></br>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="lecture-box" style="">
                <button type="button" class="lecture-collapsible"><span class="fa fa-2x fa-university text-primary"></span><span class="lecture-title">Module 6: Causality and Deep Learning (Week 4)</span></button>
                <div class="lecture-content">
                    <div class="padding-div">
                        <p class="text-muted" style="font-size: 12px"><b>Sara Magliacane</b></p>
                        <p><b>Credits: 1pts</b> <br></br> 100% of the materials is available! Note that the slides will be published soon! <br></br> This module will focus on the connections between causality and deep learning. We will start by introducing the basic concepts in causality (e.g. causal graphs, d-separations, interventions). Then we will focus on how can DL help causality, specifically for the task of causal discovery (learning the causal graph from data), which will be also the main topic of the tutorial. Finally we will discuss how can causality (or ideas from causality research) help DL and RL, focusing in particular on dealing with distribution shifts.</p>
                        <h4>Documents:</h4>
                        <div class="lecture-document-list">
                            <a ><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>DL for causality (i.e. intro to causality, causal discovery, NOTEARS, DAG-GNN, etc). (link TBA)</span></a></br>
            <a ><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Causality for ML/DL (i.e. causality for transfer learning/domain adapatation/RL, causal representation learning). (link TBA)</span></a></br>
                        </div>
                        <h4>Lecture recordings:</h4>
                        <div class="lecture-document-list">
                            <a href='https://video.uva.nl/playlist/details/0_ugmka7uv'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Causality and Deep Learning (Week 4).</span></a></br>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="lecture-box" style="margin-bottom: 40px;">
                <button type="button" class="lecture-collapsible"><span class="fa fa-2x fa-university text-primary"></span><span class="lecture-title">Module 7: High-performant Deep Learning (Week 2 & 4)</span></button>
                <div class="lecture-content">
                    <div class="padding-div">
                        <p class="text-muted" style="font-size: 12px"><b>Eric Marcus</b></p>
                        <p><b>Credits: 2pts</b> <br></br> In the high performance module, we will investigate how to use scale up your deep learning performance. By the end of this course you will be able to run your projects on (super)computers with large efficiency and minimal human intervention. We will also discuss how to track the performance of your jobs and find possible bottlenecks. The topics include: <ul> <li> Multi-GPU training: understand how to effectively distribute models and data over any amount of GPUs </li> <li>  Large scale hyperparameter tuning: launch grid or automated Bayesian searches on any number of nodes with a 'flip of a switch', and follow the results live from your laptop. </li> </ul></p>
                        <h4>Documents:</h4>
                        <div class="lecture-document-list">
                            <a ><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Fast data loading. (link TBA)</span></a></br>
            <a href='lectures/High-performant DL/dl_course_multi_gpu.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Lecture on multiGPU programming.</span></a></br>
            <a href='lectures/High-performant DL/dl_course_hyperparam.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Lecture large scale hyperparameter search (cluster based).</span></a></br>
            <a ><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Advanced topics in large models (that don't fit single devices). (link TBA)</span></a></br>
                        </div>
                        <h4>Lecture recordings:</h4>
                        <div class="lecture-document-list">
                            <a href='https://www.youtube.com/watch?v=fvucCkjqrR0'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Module 8: Lecture on multi gpu.</span></a></br>
            <a href='https://www.youtube.com/watch?v=tV4EFq5tY4g'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Lecture on hyperparameter.</span></a></br>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="lecture-box" style="">
                <button type="button" class="lecture-collapsible"><span class="fa fa-2x fa-university text-primary"></span><span class="lecture-title">Module 9: Advanced Generative Models (Week 1-2)</span></button>
                <div class="lecture-content">
                    <div class="padding-div">
                        <p class="text-muted" style="font-size: 12px"><b>Efstratios Gavves</b></p>
                        <p><b>Credits: 2pts</b> <br></br> 100% of this module's materials are available. <br></br> In this module we will study generative models beyond variational autoencoders: normalizing flows, energy-based models like scored-matching neural networks and diffusion models, Hopfield networks, and so forth.</p>
                        <h4>Documents:</h4>
                        <div class="lecture-document-list">
                            <a href='lectures/Advanced Generative & Energy-based Models/Variational autoencoders/lecture 9.1.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Variational autoencoders part 1</span></a></br>
            <a href='lectures/Advanced Generative & Energy-based Models/Variational autoencoders/lecture 9.2.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Variational autoencoders part 2</span></a></br>
            <a href='lectures/Advanced Generative & Energy-based Models/Variational autoencoders/lecture 9.3.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Variational autoencoders part 3</span></a></br>
            <a href='lectures/Advanced Generative & Energy-based Models/Variational autoencoders/lecture 9.4.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Variational autoencoders part 4<br></br></span></a></br>
            <a href='lectures/Advanced Generative & Energy-based Models/Normalizing flows and autoregressive models/lecture 11.1.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Normalizing flows part 1.</span></a></br>
            <a href='lectures/Advanced Generative & Energy-based Models/Normalizing flows and autoregressive models/lecture 11.2.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Normalizing flows part 2.</span></a></br>
            <a href='lectures/Advanced Generative & Energy-based Models/Normalizing flows and autoregressive models/lecture 11.3.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Normalizing flows part 3.</span></a></br>
            <a href='lectures/Advanced Generative & Energy-based Models/Normalizing flows and autoregressive models/lecture 11.4.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Normalizing flows part 4.</span></a></br>
            <a href='lectures/Advanced Generative & Energy-based Models/Normalizing flows and autoregressive models/lecture 11.5.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Normalizing flows part 5.  <br></br></span></a></br>
            <a href='lectures/Advanced Generative & Energy-based Models/Early energy-based models/lecture 8.1.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Early energy-based models part 1.</span></a></br>
            <a href='lectures/Advanced Generative & Energy-based Models/Early energy-based models/lecture 8.2.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Early energy-based models part 2.</span></a></br>
            <a href='lectures/Advanced Generative & Energy-based Models/Early energy-based models/lecture 8.3.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Early energy-based models part 3.</span></a></br>
            <a href='lectures/Advanced Generative & Energy-based Models/Early energy-based models/lecture 8.4.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Early energy-based models part 4<br></br>.</span></a></br>
            <a href='lectures/Advanced Generative & Energy-based Models/modern-based-models/lecture 4 - all slides.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Score-matching & Diffusion Generative Models.</span></a></br>
            <a href='lectures/Advanced Generative & Energy-based Models/modern-based-models/lecture 4.1.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Score-matching & Diffusion Generative Models - Part 1.</span></a></br>
            <a href='lectures/Advanced Generative & Energy-based Models/modern-based-models/lecture 4.2.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Score-matching & Diffusion Generative Models - Part 2.</span></a></br>
            <a href='lectures/Advanced Generative & Energy-based Models/modern-based-models/lecture 4.3.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Score-matching & Diffusion Generative Models - Part 3.</span></a></br>
            <a href='lectures/Advanced Generative & Energy-based Models/modern-based-models/lecture 4.4.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Score-matching & Diffusion Generative Models - Part 4.<br></br></span></a></br>
                        </div>
                        <h4>Lecture recordings:</h4>
                        <div class="lecture-document-list">
                            <a href='https://www.youtube.com/watch?v=zGOxAqGeI2A'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Variational autoencoders part 1.</span></a></br>
            <a href='https://www.youtube.com/watch?v=ooRFBU6pDxc'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Variational autoencoders part 2.</span></a></br>
            <a href='https://www.youtube.com/watch?v=LNE98QxUUUw'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Variational autoencoders part 3.</span></a></br>
            <a href='https://www.youtube.com/watch?v=EomjknJiHXE'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Variational autoencoders part 4.</span></a></br>
            <a href='https://www.youtube.com/watch?v=mrxLlHs2el0'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Variational autoencoders part 5. <br></br></span></a></br>
            <a href='https://www.youtube.com/watch?v=2z4IIbAlL00&t=16s'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Normilizing flows Part 1</span></a></br>
            <a href='https://www.youtube.com/watch?v=HyqW9f9ylb4&t=11s'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Normilizing flows Part 2.</span></a></br>
            <a href='https://www.youtube.com/watch?v=5nu3bO04qVQ&t=2s'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Normilizing flows Part 3.</span></a></br>
            <a href='https://www.youtube.com/watch?v=X10_deAcqDU&t=8s'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Normilizing flows Part 4.<br></br></span></a></br>
            <a href='https://www.youtube.com/watch?v=M86f6E-IVlw'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Early energy based models part 1.</span></a></br>
            <a href='https://www.youtube.com/watch?v=EgMptg_HJn4'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Early energy based models part 2.</span></a></br>
            <a href='https://www.youtube.com/watch?v=a_pAaMXfphw'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Early energy based models part 3.</span></a></br>
            <a href='https://www.youtube.com/watch?v=tzFcSeZo9J4'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Early energy based models part 4.<br></br></span></a></br>
            <a href='https://www.youtube.com/watch?v=-xr1DFlqARI'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>score matching and diffusion generative models part 1.</span></a></br>
            <a href='https://www.youtube.com/watch?v=yEC-lv1kjcE'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>score matching and diffusion generative models part 2.</span></a></br>
            <a href='https://www.youtube.com/watch?v=kJ38rkVETu0'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>score matching and diffusion generative models part 3.</span></a></br>
            <a href='https://www.youtube.com/watch?v=PiLN6ZBYreg'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>score matching and diffusion generative models part 4.</span></a></br>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="lecture-box" style="margin-bottom: 40px;">
                <button type="button" class="lecture-collapsible"><span class="fa fa-2x fa-university text-primary"></span><span class="lecture-title">Module 10: Neural Network Dynamical Systems (Week: 3)</span></button>
                <div class="lecture-content">
                    <div class="padding-div">
                        <p class="text-muted" style="font-size: 12px"><b>Efstratios Gavves</b></p>
                        <p><b>Credits: 1pts </b><br></br> In this module we will study the interface and overlap between neural networks, dynamical systems, ordinary/partial/stochastic differential equations, and physics-based neural networks. We will study how and where dynamical systems be found in neural networks with implicit functions and neural ODEs. We will also see how neural networks can be used to model dynamical systems like Navier-Stokes with physics-informed neural networks, as well as with Fourier-inspired architectures and autoregressive neural networks.</p>
                        <h4>Documents:</h4>
                        <div class="lecture-document-list">
                            <a href='lectures/neural_network_dynamical/part1.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Dynamical systems in neural networks part 1.</span></a></br>
            <a href='lectures/neural_network_dynamical/part2.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Dynamical systems for neural networks part 2.</span></a></br>
            <a href='lectures/neural_network_dynamical/part3.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Dynamical systems for neural networks part 3.</span></a></br>
            <a href='lectures/neural_network_dynamical/part4.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Dynamical systems for neural networks part 4.</span></a></br>
            <a href='lectures/neural_network_dynamical/part5.pdf'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Dynamical systems for neural networks part 5.</span></a></br>
                        </div>
                        <h4>Lecture recordings:</h4>
                        <div class="lecture-document-list">
                            <a href='https://www.youtube.com/watch?v=SfxXj5ABUPs&list=PLJ2Aod97Uj8K331mzvAFjwDhwaGH_pqfq&index=2'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Differential equations in neural networks part 1.</span></a></br>
            <a href='https://www.youtube.com/watch?v=T-EHdDjdW_4&list=PLJ2Aod97Uj8K331mzvAFjwDhwaGH_pqfq&index=1'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Differential equations in neural networks part 2.</span></a></br>
            <a href='https://www.youtube.com/watch?v=_ZGqNzj2zII&list=PLJ2Aod97Uj8K331mzvAFjwDhwaGH_pqfq&index=3'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Differential equations in neural networks part 3.</span></a></br>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="lecture-box" style="">
                <button type="button" class="lecture-collapsible"><span class="fa fa-2x fa-university text-primary"></span><span class="lecture-title">Module 11: Sampling & Gradient Approximations (Week: 3-4)</span></button>
                <div class="lecture-content">
                    <div class="padding-div">
                        <p class="text-muted" style="font-size: 12px"><b>Efstratios Gavves</b></p>
                        <p><b>Credits: 2pts </b><br></br>  The materials for this module will be added soon! <br></br> In this module we will study how one can integrate complex structure into neural networks. We will start with a quick introduction to Monte-Carlo simulation for neural networks. We will discuss Gumbel-Softmax  for differentiable models and the Gumbel-Argmax trick for sampling from categorical distributions, as well as then the Gumbel-Straight-Through variation. We will continue with subset sampling with Gumbel-Topk relaxations, sampling graphs in latent spaces in relational inference with GNN encoder/decode, as well as asampling latent permutations with Gumbel-Sinkhorn. Last, we will discuss about how discrete and continuous sampling connects to analysis of functions from mathematics.</p>
                        <h4>Documents:</h4>
                        <div class="lecture-document-list">
                            <a ><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Deep sampling, variance reduction. (link TBA)</span></a></br>
            <a ><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Gumbel, straight-through, Harmonic analysis. (link TBA)</span></a></br>
            <a ><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Sampling structures. (link TBA)</span></a></br>
                        </div>
                        <h4>Lecture recordings:</h4>
                        <div class="lecture-document-list">
                            No recordings.
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="lecture-box" style="margin-bottom: 40px;">
                <button type="button" class="lecture-collapsible"><span class="fa fa-2x fa-university text-primary"></span><span class="lecture-title">Module 12: Neural Information Retrieval (Week: 3)</span></button>
                <div class="lecture-content">
                    <div class="padding-div">
                        <p class="text-muted" style="font-size: 12px"><b>Andrew Yates</b></p>
                        <p><b> Credits: 1pts </b><br></br> Neural Information Retrieval is the application of deep learning models to ranking tasks, such as ranking documents by their relevance to a given query. After briefly introducing IR basics, this module will cover the three families of models commonly used for this task and their trade-offs.</p>
                        <h4>Documents:</h4>
                        <div class="lecture-document-list">
                            <a href='https://arxiv.org/abs/2010.06467'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Pretrained Transformers for Text Ranking: BERT and Beyond</span></a></br>
                        </div>
                        <h4>Lecture recordings:</h4>
                        <div class="lecture-document-list">
                            <a href=' https://youtu.be/WAt0GxQDSkc'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Neural IR part 1.</span></a></br>
            <a href=' https://www.youtube.com/watch?v=VzTzMYgpyVk'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Neural IR part 2.</span></a></br>
                        </div>
                    </div>
                </div>
            </div>

        </div>
    </section>

    <div class="divider"></div>

    <section id="practicals">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Tutorials</h2>
                    <hr class="primary">
                </div>
            </div>

            <div class="lecture-box">
                <button type="button" class="lecture-collapsible"><span class="fa fa-2x fa-desktop text-primary"></span><span class="lecture-title">Module: Geometric deep learning</span></button>
                <div class="lecture-content">
                    <div class="padding-div">
                        <table><tr><td style="width: 100%; margin: auto;">
                            <p class="text-muted" style="font-size: 12px">Deadline: <!--$$DEADLINE$$--></p>
                            <p> This module covers the topic of geometric deep learning, touching upon all its five G's (Grids, Groups, Graphs, Geodesics, and Gauges) but with a strong focus on group equivariant deep learning. The impact that CNNs made in fields such as computer vision, computational chemistry and physics, can largely be attributed to the fact that convolutions allow for weight sharing, geometric stability, and a dramatic decrease in learnable parameters by leveraging symmetries in data and architecture design. These enabling properties arise from the equivariance property of convolutions. That is, if the input image is translated, the output of a convolution is translated accordingly, which in turn means that local information does not get lost in the neural network upon an input transformation (it is just shifted to a different location). With group equivariant deep learning we can hard-code stability and weight sharing over transformations beyond just translations. E.g., it allows for sharing of weights (representing complex patterns/representations) over poses and symmetries represented by transformations such as translation + rotation + scaling.</p>
                            <h4>Tutorials:</h4>
                            <div class="lecture-document-list">
                                <a href='https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/Geometric_deep_learning/tutorial1_regular_group_convolutions.html'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Regular Group Convolutions</span></a></br>
            <a href='https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/Geometric_deep_learning/tutorial2_steerable_cnns.html'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>GDL - Steerable CNNs</span></a></br>
            <a href='https://colab.research.google.com/drive/1_INZ8cnQ1sotd8FEhD1-K4JIOYtMzJO7?usp=sharing'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Equivariant graph tutorial</span></a></br>
            <a href='https://colab.research.google.com/drive/1_INZ8cnQ1sotd8FEhD1-K4JIOYtMzJO7?usp=sharing'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Equivariant graph tutorial (without solutions)</span></a></br>
            <a href='https://colab.research.google.com/drive/1XB35GhYkbo_vjc6SnA8Usko0tdpzgSIa?usp=sharing'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Geometric Latent Spaces Tutorial</span></a></br>
                            </div>
                            <h4>Lecture recordings:</h4>
                            <div class="lecture-document-list">
                                <a href='https://www.youtube.com/watch?v=lGzY5m91_es&list=PLJ2Aod97Uj8JQX48cBwAjhabxhLu8Etht&index=2'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Q&A session for Geometric Deep learning Tutorial 2.</span></a></br>
            <a href='https://www.youtube.com/watch?v=KRX-2GQ2fL0&list=PLJ2Aod97Uj8JQX48cBwAjhabxhLu8Etht&index=3&ab_channel=DeepLearningII'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>Geometric Deep learning -regular gcnns tutorial.</span></a></br>
                            </div>
                        </td><td>
                            <img src="<!--$$IMAGE$$-->" alt="" width="300" style="padding-bottom: 10px"></br>
                            <center class="text-muted" style="font-size: 12px"><!--$$IMAGE_DESC$$--></center>
                        </td></tr></table>
                    </div>
                </div>
            </div>
            
            <div class="lecture-box">
                <button type="button" class="lecture-collapsible"><span class="fa fa-2x fa-desktop text-primary"></span><span class="lecture-title">Module: Bayesian deep learning</span></button>
                <div class="lecture-content">
                    <div class="padding-div">
                        <table><tr><td style="width: 100%; margin: auto;">
                            <p class="text-muted" style="font-size: 12px">Deadline: <!--$$DEADLINE$$--></p>
                            <p>Usually we train neural networks (NNs) by way of point estimation.  After running gradient descent, each weight is represented by a particular value.  In this module, we consider giving NNs a Bayesian treatment.  We start by specifying a probability distribution for each weight---p(W)---and the goal of training is to obtain the posterior distribution p(W | D), where D denotes the training data, via Bayes rule.  The Bayesian approach provides natural mechanisms for model selection, complexity control, incorporating prior knowledge, uncertainty quantification, and online learning.  Yet, Bayesian inference is computationally demanding, and much of the module will focus on scalable methods for estimating the posterior distribution.</p>
                            <h4>Tutorials:</h4>
                            <div class="lecture-document-list">
                                <a href='https://colab.research.google.com/drive/1D8I1Cc0690IKs3ye8dyoXen7r_gftirI?usp=sharing'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Tutorial 1: Bayesian NNs with Pyro (with solutions)</span></a></br>
            <a href='https://colab.research.google.com/drive/15NcA71SOPuOL67yBtosO0jmtVXanFaOX#scrollTo=U2aP1KBC6x2V'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Tutorial 1: Bayesian NNs with Pyro (without solutions)</span></a></br>
            <a href='https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/Bayesian_Neural_Networks/Complete_DLII_BNN_2_2.html'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Tutorial 2: Comparing to Non-Bayesian Methods for Uncertainty (with solutions)</span></a></br>
            <a href='https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/DL2/Bayesian_Neural_Networks/Student_DLII_BNN_2_2.ipynb'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Tutorial 2: Comparing to Non-Bayesian Methods for Uncertainty (without solutions)</span></a></br>
                            </div>
                            <h4>Lecture recordings:</h4>
                            <div class="lecture-document-list">
                                Recordings will be added soon.
                            </div>
                        </td><td>
                            <img src="<!--$$IMAGE$$-->" alt="" width="300" style="padding-bottom: 10px"></br>
                            <center class="text-muted" style="font-size: 12px"><!--$$IMAGE_DESC$$--></center>
                        </td></tr></table>
                    </div>
                </div>
            </div>
            
            <div class="lecture-box">
                <button type="button" class="lecture-collapsible"><span class="fa fa-2x fa-desktop text-primary"></span><span class="lecture-title">Module: Deep probabilistic models I</span></button>
                <div class="lecture-content">
                    <div class="padding-div">
                        <table><tr><td style="width: 100%; margin: auto;">
                            <p class="text-muted" style="font-size: 12px">Deadline: <!--$$DEADLINE$$--></p>
                            <p>In this module you learn to view data as a byproduct of probabilistic experiments. You will parameterise joint probability distributions over observed random variables and perform parameter estimation by regularised gradient-based maximum likelihood estimation</p>
                            <h4>Tutorials:</h4>
                            <div class="lecture-document-list">
                                <a href='https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/deep_probabilistic_models_I/tutorial_1.html'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>DPM1 - Deep Probabilistic Models I</span></a></br>
                            </div>
                            <h4>Lecture recordings:</h4>
                            <div class="lecture-document-list">
                                No recordings.
                            </div>
                        </td><td>
                            <img src="<!--$$IMAGE$$-->" alt="" width="300" style="padding-bottom: 10px"></br>
                            <center class="text-muted" style="font-size: 12px"><!--$$IMAGE_DESC$$--></center>
                        </td></tr></table>
                    </div>
                </div>
            </div>
            
            <div class="lecture-box">
                <button type="button" class="lecture-collapsible"><span class="fa fa-2x fa-desktop text-primary"></span><span class="lecture-title">Module: Deep probabilistic models II</span></button>
                <div class="lecture-content">
                    <div class="padding-div">
                        <table><tr><td style="width: 100%; margin: auto;">
                            <p class="text-muted" style="font-size: 12px">Deadline: <!--$$DEADLINE$$--></p>
                            <p>In this session you learn to model complex data along with unobserved discrete random variables. Examples you are probably already familiar with include mixture models, factor models, and HMMs. You will learn how to approximate intractable inference using stochastic variational methods and derive efficient gradient estimators for parameter estimation via backpropagation.</p>
                            <h4>Tutorials:</h4>
                            <div class="lecture-document-list">
                                <a href='https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/deep_probabilistic_models_II/tutorial_2a.html'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>DPM2 - Deep Probabilistic Models II</span></a></br>
                            </div>
                            <h4>Lecture recordings:</h4>
                            <div class="lecture-document-list">
                                No recordings.
                            </div>
                        </td><td>
                            <img src="<!--$$IMAGE$$-->" alt="" width="300" style="padding-bottom: 10px"></br>
                            <center class="text-muted" style="font-size: 12px"><!--$$IMAGE_DESC$$--></center>
                        </td></tr></table>
                    </div>
                </div>
            </div>
            
            <div class="lecture-box">
                <button type="button" class="lecture-collapsible"><span class="fa fa-2x fa-desktop text-primary"></span><span class="lecture-title">Module: High-performant Deep Learning</span></button>
                <div class="lecture-content">
                    <div class="padding-div">
                        <table><tr><td style="width: 100%; margin: auto;">
                            <p class="text-muted" style="font-size: 12px">Deadline: <!--$$DEADLINE$$--></p>
                            <p>In the high performance module, we will investigate how to use scale up your deep learning performance. By the end of this course you will be able to run your projects on (super)computers with large efficiency and minimal human intervention. We will also discuss how to track the performance of your jobs and find possible bottlenecks. The topics include: <ul> <li> Multi-GPU training: understand how to effectively distribute models and data over any amount of GPUs </li> <li>  Large scale hyperparameter tuning: launch grid or automated Bayesian searches on any number of nodes with a 'flip of a switch', and follow the results live from your laptop.</p>
                            <h4>Tutorials:</h4>
                            <div class="lecture-document-list">
                                <a href='https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/High-performant_DL/hyperparameter_search/hpdlhyperparam.html'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Introduction to HyperParameter Tuning</span></a></br>
            <a href='https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/High-performant_DL/Multi_GPU/hpdlmultigpu.html'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Introduction to Multi GPU Programming</span></a></br>
                            </div>
                            <h4>Lecture recordings:</h4>
                            <div class="lecture-document-list">
                                <a href='https://www.youtube.com/watch?v=ZZFoCuiTbC4&list=PLJ2Aod97Uj8JQX48cBwAjhabxhLu8Etht&ab_channel=DeepLearningII'target="_blank"><i class='fa fa fa-video-camera text-primary'></i><span class='lecture-document'>High Performance Computing Tutorial.</span></a></br>
                            </div>
                        </td><td>
                            <img src="<!--$$IMAGE$$-->" alt="" width="300" style="padding-bottom: 10px"></br>
                            <center class="text-muted" style="font-size: 12px"><!--$$IMAGE_DESC$$--></center>
                        </td></tr></table>
                    </div>
                </div>
            </div>
            
            <div class="lecture-box">
                <button type="button" class="lecture-collapsible"><span class="fa fa-2x fa-desktop text-primary"></span><span class="lecture-title">Module: Advanced generative models</span></button>
                <div class="lecture-content">
                    <div class="padding-div">
                        <table><tr><td style="width: 100%; margin: auto;">
                            <p class="text-muted" style="font-size: 12px">Deadline: <!--$$DEADLINE$$--></p>
                            <p> In this module we will study generative models beyond variational autoencoders: normalizing flows, energy-based models like scored-matching neural networks and diffusion models, Hopfield networks, and so forth.</p>
                            <h4>Tutorials:</h4>
                            <div class="lecture-document-list">
                                <a href='https://colab.research.google.com/drive/1ynZQKEC4_C9Xs4hhlyqOSKxqOTBJiTFE?usp=sharing'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Energy-based Models</span></a></br>
            <a href='https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/Advanced_Generative_Models/Normalizing_flows/advancednormflow.html'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Normilizing flows</span></a></br>
                            </div>
                            <h4>Lecture recordings:</h4>
                            <div class="lecture-document-list">
                                No recordings.
                            </div>
                        </td><td>
                            <img src="<!--$$IMAGE$$-->" alt="" width="300" style="padding-bottom: 10px"></br>
                            <center class="text-muted" style="font-size: 12px"><!--$$IMAGE_DESC$$--></center>
                        </td></tr></table>
                    </div>
                </div>
            </div>
            
            <div class="lecture-box">
                <button type="button" class="lecture-collapsible"><span class="fa fa-2x fa-desktop text-primary"></span><span class="lecture-title">Module: Neural Network Dynamical Systems</span></button>
                <div class="lecture-content">
                    <div class="padding-div">
                        <table><tr><td style="width: 100%; margin: auto;">
                            <p class="text-muted" style="font-size: 12px">Deadline: <!--$$DEADLINE$$--></p>
                            <p>In this module we will study the interface and overlap between neural networks, dynamical systems, ordinary/partial/stochastic differential equations, and physics-based neural networks. We will study how and where dynamical systems be found in neural networks with implicit functions and neural ODEs. We will also see how neural networks can be used to model dynamical systems like Navier-Stokes with physics-informed neural networks, as well as with Fourier-inspired architectures and autoregressive neural networks.</p>
                            <h4>Tutorials:</h4>
                            <div class="lecture-document-list">
                                <a href='https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/Dynamical_systems/dynamical_systems_neural_odes.html'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>DS - Dynamical Systems & Neural ODEs.</span></a></br>
                            </div>
                            <h4>Lecture recordings:</h4>
                            <div class="lecture-document-list">
                                No recordings.
                            </div>
                        </td><td>
                            <img src="<!--$$IMAGE$$-->" alt="" width="300" style="padding-bottom: 10px"></br>
                            <center class="text-muted" style="font-size: 12px"><!--$$IMAGE_DESC$$--></center>
                        </td></tr></table>
                    </div>
                </div>
            </div>
            
            <div class="lecture-box">
                <button type="button" class="lecture-collapsible"><span class="fa fa-2x fa-desktop text-primary"></span><span class="lecture-title">Module:  Neural Information Retrieval</span></button>
                <div class="lecture-content">
                    <div class="padding-div">
                        <table><tr><td style="width: 100%; margin: auto;">
                            <p class="text-muted" style="font-size: 12px">Deadline: <!--$$DEADLINE$$--></p>
                            <p>  Neural Information Retrieval is the application of deep learning models to ranking tasks, such as ranking documents by their relevance to a given query. After briefly introducing IR basics, this module will cover the three families of models commonly used for this task and their trade-offs.</p>
                            <h4>Tutorials:</h4>
                            <div class="lecture-document-list">
                                <a href='https://colab.research.google.com/drive/11Tco2NSDf-9zv3OsjKx_PXDiyYbLI3pA?usp=sharing'target="_blank"><i class='fa fa fa-file-text-o text-primary'></i><span class='lecture-document'>Tutorial</span></a></br>
                            </div>
                            <h4>Lecture recordings:</h4>
                            <div class="lecture-document-list">
                                No recordings.
                            </div>
                        </td><td>
                            <img src="<!--$$IMAGE$$-->" alt="" width="300" style="padding-bottom: 10px"></br>
                            <center class="text-muted" style="font-size: 12px"><!--$$IMAGE_DESC$$--></center>
                        </td></tr></table>
                    </div>
                </div>
            </div>

        </div>

    </section>


    <section id="course-links" class="bg-dark">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 text-center">
                    <h2 class="section-heading">Links</h2>
                    <hr class="primary">
                    <p style="text-align:justify; font-size:18px;">Some useful links for the course are the following:

                    <p>
                    <div align="left">

                    <ul>

                    <li> <a href="https://piazza.com/class/l136xghmlo16ai" target="_blank" style="font-size:16px;"> Piazza </a></li>
                    
                    <li> <a href="https://datanose.nl/#course[99881]" target="_blank" style="font-size:16px;"> Datanose </a></li>

                    <li> <a href="https://canvas.uva.nl/courses/28689" target="_blank" style="font-size:16px;"> Canvas </a></li>

                    <!-- <li> <a href="https://probabll.github.io/teaching/probdl/," target="_blank" style="font-size:16px;"> Deep probabilistic models </a></li> -->

                    <li> <a href="https://www.youtube.com/channel/UCxf_K59uNpwBs-O67MT50-A/playlists," target="_blank" style="font-size:16px;"> YouTube channel </a></li>
                    </ul>

                    </div>
                    </p>

                </div>
            </div>
        </div>
    </section>

    <!-- <section class="bg-dark" id="old-lectures">
        <div class="container text-center">
            <div class="call-to-action">
                <h2>If you are interested in older versions of the lectures, you can find them below.</h2> -->

                <!-- <a href="lectures-feb2016.html#" class="btn btn-default btn-xl wow" target="_blank">UVADLC Feb 2016</a>
                <a href="lectures-nov2016.html#" class="btn btn-default btn-xl wow" target="_blank">UVADLC Nov 2016</a>
                <a href="lectures-nov2017.html#" class="btn btn-default btn-xl wow" target="_blank">UVADLC Nov 2017</a>
                <a href="lectures-sep2018.html#" class="btn btn-default btn-xl wow" target="_blank">UVADLC Sep 2018</a>
                <a href="lectures-apr2019.html#" class="btn btn-default btn-xl wow" target="_blank">UVADLC Apr 2019</a>
                <a href="lectures-nov2019.html#" class="btn btn-default btn-xl wow" target="_blank">UVADLC Nov 2019</a>
                <a href="lectures-nov2020.html#" class="btn btn-default btn-xl wow" target="_blank">UVADLC Nov 2020</a>
            </div>
        </div>
    </section> -->

    <section id="contact">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 text-center">
                    <h2 class="section-heading">Contact us!</h2>
                    <hr class="primary">
                    <p class="running-text" style="text-align:justify">
                    If you have any questions or recommendations for the website or the course, you can always drop us a line!
                    The knowledge should be free, so feel also free to use any of the material provided here (but please be so kind to cite us).
                    In case you are a course instuctor and you want the solutions, please send us an email.
                    </p>
                </div>
                <div class="col-lg-4 col-lg-offset-2 text-center">
                    <i class="fa fa-map-marker fa-3x wow bounceIn"></i>
                    <p><a href="https://goo.gl/maps/HzB8CMdkTDA2">Room C3.229, Science Park 904 1098 XH, Amsterdam, The Netherlands</a></p>
                </div>
                <div class="col-lg-4 text-center">
                    <i class="fa fa-envelope-o fa-3x wow bounceIn" data-wow-delay=".1s"></i>
                    <p><a href="mailto:e.gavves@uva.nl">e.gavves@uva.nl</a></p>
                </div>
            </div>
        </div>
    </section>

    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="js/jquery.easing.min.js"></script>
    <script src="js/jquery.fittext.js"></script>
    <script src="js/wow.min.js"></script>

    <!-- Custom Theme JavaScript -->
    <script src="js/creative.js"></script>
    <script src="js/collapsible.js"></script>

</body>

</html>
