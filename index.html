<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>UvA Deep Learning II Course</title>

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="css/bootstrap.min.css" type="text/css">

    <!-- Custom Fonts -->
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Merriweather:400,300,300italic,400italic,700,700italic,900,900italic' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="font-awesome/css/font-awesome.min.css" type="text/css">

    <!-- Plugin CSS -->
    <link rel="stylesheet" href="css/animate.min.css" type="text/css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="css/creative.css" type="text/css">

    <link rel="apple-touch-icon" sizes="57x57" href="icons/apple-icon-57x57.png">
    <link rel="apple-touch-icon" sizes="60x60" href="icons/apple-icon-60x60.png">
    <link rel="apple-touch-icon" sizes="72x72" href="icons/apple-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="76x76" href="icons/apple-icon-76x76.png">
    <link rel="apple-touch-icon" sizes="114x114" href="icons/apple-icon-114x114.png">
    <link rel="apple-touch-icon" sizes="120x120" href="icons/apple-icon-120x120.png">
    <link rel="apple-touch-icon" sizes="144x144" href="icons/apple-icon-144x144.png">
    <link rel="apple-touch-icon" sizes="152x152" href="icons/apple-icon-152x152.png">
    <link rel="apple-touch-icon" sizes="180x180" href="icons/apple-icon-180x180.png">
    <link rel="icon" type="image/png" sizes="192x192"  href="icons/android-icon-192x192.png">
    <link rel="icon" type="image/png" sizes="32x32" href="icons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="96x96" href="icons/favicon-96x96.png">
    <link rel="icon" type="image/png" sizes="16x16" href="icons/favicon-16x16.png">
    <link rel="manifest" href="/manifest.json">
    <meta name="msapplication-TileColor" content="#ffffff">
    <meta name="msapplication-TileImage" content="icons/ms-icon-144x144.png">
    <meta name="theme-color" content="#ffffff">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-86672135-1', 'auto');
  ga('send', 'pageview');

</script>

<body id="page-top">

    <nav id="mainNav" class="navbar navbar-default navbar-fixed-top">
        <div class="container-fluid">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand page-scroll" href="#page-top">UvA Deep Learning II Course</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a class="page-scroll" href="#about">About</a>
                    </li>

                    <li>
                        <a class="page-scroll" href="#lectures">Modules</a>
                    </li>

                    <li>
                        <a class="page-scroll" href="#practicals">Tutorials</a>
                    </li>

                    <li>
                        <a class="page-scroll" href="#old-lectures">Old materials</a>
                    </li>
                        
                   
                    <li>
                        <a class="page-scroll" href="#course-links">Links</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#contact">Contact</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container-fluid -->
    </nav>

    <header>
        <div class="header-content">
            <div class="header-content-inner">
                <h1>UVA Deep Learning II Course</h1>
                <hr>
                <p>MSc in Artificial Intelligence for the University of Amsterdam.</p>
                <a href="#about" class="btn btn-primary btn-xl page-scroll">Find Out More</a>
            </div>
        </div>
    </header>

    <section class="bg-primary" id="about">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 text-center">
                    <h2 class="section-heading">About</h2>
                    <hr class="light">
                    <p class="text-faded" style="text-align:justify">Deep learning II is taught in the MSc program in Artificial Intelligence of the University of Amsterdam. In this course we study the theory of deep learning, namely of modern, multi-layered neural networks trained on big data. The course is coordinated by <a href="mailto: e.gavves@uva.nl" style="color: white;">Efstratios Gavves</a>, <a href="mailto: e.j.bekkers@uva.nl" style="color: white;">Erik Bekkers</a>, <a href="mailto: W.FerreiraAziz@uva.nl" style="color: white;">Wilker Aziz Fereira </a> and <a href="mailto: c.athanasiadis@uva.nl" style="color: white;">Christos Athanasiadis</a>.
                        
                        <br></br>
                        <a href="mailto: e.gavves@uva.nl""><img class="img-circle" src="images/people/efstratios-gavves.png" hspace="5" width="120" title="Efstratios Gavves"></a>
                            <a href="mailto: W.FerreiraAziz@uva.nl""><img class="img-circle" src="images/people/wilker-aziz-fereira.png" hspace="5" width="120" title="Wilker-Aziz-Fereira"></a>
                            <a href="mailto: e.j.bekkers@uva.nl""><img class="img-circle" src="images/people/erik.jpg" hspace="5" width="120" title="Erik Bekkers"></a>
                            <a href="mailto: c.athanasiadis@uva.nl""><img class="img-circle" src="images/people/christos-athanasiadis.jpg" hspace="5" width="120" title="Christos Athanasiadis"></a>
                        <br></br>
                            And the lectures for the course are: Erik Bekkers, Eric Nalisnick, Sara Magliacane, Yuki Asano, Wilker Aziz Ferreira and Stratis Gaves. 
                            <br></br>
                            <a href="mailto: e.gavves@uva.nl""><img class="img-circle" src="images/people/efstratios-gavves.png" hspace="5" width="120" title="Efstratios Gavves"></a>
                            <a href="mailto: W.FerreiraAziz@uva.nl""><img class="img-circle" src="images/people/wilker-aziz-fereira.png" hspace="5" width="120" title="Wilker-Aziz-Fereira"></a>
                            <a href="mailto: e.j.bekkers@uva.nl""><img class="img-circle" src="images/people/erik.jpg" hspace="5" width="120" title="Erik Bekkers"></a>
                            <a href="mailto: e.t.nalisnick@uva.nl""><img class="img-circle" src="images/people/eric.jpg" hspace="5" width="120" title="Eric Nalisnick"></a>
                            <a href="mailto: s.magliacane@uva.nl""><img class="img-circle" src="images/people/sara.jpg" hspace="5" width="120" title="Sara Magliacane"></a>
                            <a href="mailto: y.m.asano@uva.nl""><img class="img-circle" src="images/people/yuki.jpg" hspace="5" width="120" title="Yuki Asano"></a>

                        <br></br> 
                    The Teaching Assistants (TAs) are:
                    <a href='mailto:c.athanasiadis@uva.nl' target='_blank' style='color: white;'>Christos Athanasiadis</a>, <a href='mailto:l.f.bereska@uva.nl' target='_blank' style='color: white;'>Leonard Bereska</a>, <a href='mailto:m.m.derakhshani@uva.nl' target='_blank' style='color: white;'>Mohammad Mahdi Derakhshani</a>, <a href='mailto:b.eikema@uva.nl' target='_blank' style='color: white;'>Bryan Eikema</a>, <a href='mailto:danilogoede@gmail.com' target='_blank' style='color: white;'>Danilo De Goede</a>, <a href='mailto:s.h.hsu@uva.nl' target='_blank' style='color: white;'>Cyril Hsu</a>, <a href='mailto:d.m.knigge@uva.nl' target='_blank' style='color: white;'>David Knigge</a>, <a href='mailto:m.kofinas@uva.nl' target='_blank' style='color: white;'>Miltos Kofinas</a>, <a href='mailto:p.a.vanderlinden@uva.nl' target='_blank' style='color: white;'>Putri Linden</a>, <a href='mailto:c.liu4@uva.nl' target='_blank' style='color: white;'>Cong Liu</a>, <a href='mailto:i.najdenkoska@uva.nl' target='_blank' style='color: white;'>Ivona Najdenkoska</a>, <a href='mailto:angelos.nalmpantis@student.uva.nl' target='_blank' style='color: white;'>Angelos Nalmpantis</a>, <a href='mailto:s.papa@uva.nl' target='_blank' style='color: white;'>Samuel Papa</a>, <a href='mailto:d.ruhe@uva.nl' target='_blank' style='color: white;'>David Ruhe</a>, <a href='mailto:r.valperga@uva.nl' target='_blank' style='color: white;'>Riccardo Valperga</a>
                </p>
                   

                    <br></br>
                    <a href='mailto:c.athanasiadis@uva.nl'><img class='img-circle' src='images/people/christos-athanasiadis.jpg' hspace='5' width='120' alt='Christos Athanasiadis' title='Christos Athanasiadis'></a>
<a href='mailto:l.f.bereska@uva.nl'><img class='img-circle' src='images/people/leonard-bereska.jpg' hspace='5' width='120' alt='Leonard Bereska' title='Leonard Bereska'></a>
<a href='mailto:m.m.derakhshani@uva.nl'><img class='img-circle' src='images/people/mohammad-mahdi-derakhshani.jpg' hspace='5' width='120' alt='Mohammad Mahdi Derakhshani' title='Mohammad Mahdi Derakhshani'></a>
<a href='mailto:b.eikema@uva.nl'><img class='img-circle' src='images/people/bryan-eikema.jpg' hspace='5' width='120' alt='Bryan Eikema' title='Bryan Eikema'></a></br></br>
<a href='mailto:danilogoede@gmail.com'><img class='img-circle' src='images/people/danilo-de-goede.jpg' hspace='5' width='120' alt='Danilo De Goede' title='Danilo De Goede'></a>
<a href='mailto:s.h.hsu@uva.nl'><img class='img-circle' src='images/people/cyril-hsu.jpg' hspace='5' width='120' alt='Cyril Hsu' title='Cyril Hsu'></a>
<a href='mailto:d.m.knigge@uva.nl'><img class='img-circle' src='images/people/david-knigge.jpg' hspace='5' width='120' alt='David Knigge' title='David Knigge'></a>
<a href='mailto:m.kofinas@uva.nl'><img class='img-circle' src='images/people/miltos-kofinas.jpg' hspace='5' width='120' alt='Miltos Kofinas' title='Miltos Kofinas'></a>
<a href='mailto:p.a.vanderlinden@uva.nl'><img class='img-circle' src='images/people/putri-linden.jpg' hspace='5' width='120' alt='Putri Linden' title='Putri Linden'></a></br></br>
<a href='mailto:c.liu4@uva.nl'><img class='img-circle' src='images/people/cong-liu.jpg' hspace='5' width='120' alt='Cong Liu' title='Cong Liu'></a>
<a href='mailto:i.najdenkoska@uva.nl'><img class='img-circle' src='images/people/ivona-najdenkoska.jpg' hspace='5' width='120' alt='Ivona Najdenkoska' title='Ivona Najdenkoska'></a>
<a href='mailto:angelos.nalmpantis@student.uva.nl'><img class='img-circle' src='images/people/angelos-nalmpantis.jpg' hspace='5' width='120' alt='Angelos Nalmpantis' title='Angelos Nalmpantis'></a>
<a href='mailto:s.papa@uva.nl'><img class='img-circle' src='images/people/samuel-papa.jpg' hspace='5' width='120' alt='Samuel Papa' title='Samuel Papa'></a>
<a href='mailto:d.ruhe@uva.nl'><img class='img-circle' src='images/people/david-ruhe.jpg' hspace='5' width='120' alt='David Ruhe' title='David Ruhe'></a></br></br>
<a href='mailto:r.valperga@uva.nl'><img class='img-circle' src='images/people/riccardo-valperga.jpg' hspace='5' width='120' alt='Riccardo Valperga' title='Riccardo Valperga'></a>
                </div>
            </div>
        </div>
    </section>

    <section id="lectures">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Modules</h2>
                    <hr class="primary">
                </div>
            </div>

            <div class="lecture-box" style="margin-bottom: 40px;">
                <button type="button" class="lecture-collapsible"><span class="fa fa-2x fa-university text-primary"></span><span class="lecture-title">Module 1: Group equivariant deep learning</span></button>
                <div class="lecture-content">
                    <div class="padding-div">
                        <p class="text-muted" style="font-size: 12px"><b>Erik Bekkers </b></p>
                        <p><b>Credits: 2pts</b> <br></br> This module covers the topic of geometric deep learning, touching upon all its five G's (Grids, Groups, Graphs, Geodesics, and Gauges) but with a strong focus on group equivariant deep learning. The impact that CNNs made in fields such as computer vision, computational chemistry and physics, can largely be attributed to the fact that convolutions allow for weight sharing, geometric stability, and a dramatic decrease in learnable parameters by leveraging symmetries in data and architecture design. These enabling properties arise from the equivariance property of convolutions. In this module you will learn how to equip neural networks with equivariance properties. The module is split in 4 lectures with accompanying tutorials: This module is split into 4 lectures: <ul><li> <b>Lecture 1 (Wednesday 6th of April on-site Q&A)</b>: Regular group convolutional neural networks (G-CNNs). In this lecture we cover the basics of group convolutional NNs and show how to leverage symmetries in data and practical problems.</li>  <li> <b>Lecture 2 (Tuesday 12th of April on-site Q&A)</b>: Steerable G-CNNs. In this lecture we introduce a very general class of G-CNNs that allows to handle (rotational) symmetries in a flexible and powerful way. These methods are at the core of the most successful methods to handle 3D data such as atomic point clouds, but are also at the core of gauge equivariant methods that are applicable to arbitrary Riemannian manifolds.</li> <li> <b>Lecture 3 (Tuesday 19th of April on-site Q&A)</b>: Equivariant graph NNs. Many problems in computational chemistry and computational physics are now-a-days solved via graph NNs. The SotA in these domains derive their effectives from the geometric structure and symmetries presented by the data and underlying physics. In this lecture cover tools for SE(3) equivariance in the context of state-of-the-art in geometric graph NNs. </li> <li> <b>Lecture 4 (Tuesday 26th of April on-site Q&A)</b>: Recap and/or, if time allows, further exploration of topics covered in this module (e.g. equivariant transformers, geometric latent spaces, â€¦). </li> </ul></p>
                        <h4>Documents:</h4>
                        <div class="lecture-document-list">
                            No documents.
                        </div>
                        <h4>Lecture recordings:</h4>
                        <div class="lecture-document-list">
                            No recordings.
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="lecture-box" style="">
                <button type="button" class="lecture-collapsible"><span class="fa fa-2x fa-university text-primary"></span><span class="lecture-title">Module 2: Bayesian Deep Learning</span></button>
                <div class="lecture-content">
                    <div class="padding-div">
                        <p class="text-muted" style="font-size: 12px"><b> Eric Nalisnick </b></p>
                        <p><b>Credits: 1pts </b><br></br> For principled decision making, it is usually not enough to have our models produce only point predictions.  Safety-critical scenarios require tracking and communicating the corresponding uncertainty in that prediction. These uncertainties are then useful for, among other things, knowing when a system should not be trusted. This could be because the system is operating in anomalous conditions or simply because a decision is inherently difficult. In either case, if the uncertainty outputs are sufficiently high, we could stop the system and call upon a human for help. For example, in the setting of autonomous driving, this could mean passing off control to a human driver. Other options would be to slow down the car, collect more data, activate other sensors, etc. In modern computer vision systems based off deep learning, quantifying uncertainty is especially challenging since many classical techniques (e.g. asymptotic distribution of an estimator) cannot be applied. Thus there is often an inherent tradeoff between exploiting the predictive power of deep learning and having principled uncertainty estimates.  This module explores the tension between principled uncertainty quantification and computational tractability, covering Bayesian neural networks, scalable approximate inference, and recent alternatives such as conformal inference. <ul><li> <b>Lecture 1 (Wednesday 6th of April on-site Q&A)</b>: </li><li> <b>Lecture 2 (Wednesday 6th of April on-site Q&A)</b>: </li></ul></p>
                        <h4>Documents:</h4>
                        <div class="lecture-document-list">
                            No documents.
                        </div>
                        <h4>Lecture recordings:</h4>
                        <div class="lecture-document-list">
                            No recordings.
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="lecture-box" style="margin-bottom: 40px;">
                <button type="button" class="lecture-collapsible"><span class="fa fa-2x fa-university text-primary"></span><span class="lecture-title">Module 3: Deep probabilistic models</span></button>
                <div class="lecture-content">
                    <div class="padding-div">
                        <p class="text-muted" style="font-size: 12px"><b>Wilker Aziz Ferreira</b></p>
                        <p><b>Credits: 1pts</b><br></br> In this module you learn to view data as a byproduct of probabilistic experiments. You will parameterise joint probability distributions over observed random variables, however complex/structured they may be, and perform parameter estimation by regularised gradient-based maximum likelihood estimation.  <ul><li> <b>Lecture 1</b>:   </li> <li> <b>Lecture 2</b>:   </li></ul></p>
                        <h4>Documents:</h4>
                        <div class="lecture-document-list">
                            No documents.
                        </div>
                        <h4>Lecture recordings:</h4>
                        <div class="lecture-document-list">
                            No recordings.
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="lecture-box" style="">
                <button type="button" class="lecture-collapsible"><span class="fa fa-2x fa-university text-primary"></span><span class="lecture-title">Module 4: Causality and causal representation learning</span></button>
                <div class="lecture-content">
                    <div class="padding-div">
                        <p class="text-muted" style="font-size: 12px"><b>Sara Magliacane</b></p>
                        <p><b>Credits: 1pts</b> <br></br> This module will focus on the connections between causality and deep learning. We will start by introducing the basic concepts in causality (e.g. causal graphs, d-separations, interventions). Then we will focus on how can DL help causality, specifically for the task of causal discovery (learning the causal graph from data), which will be also the main topic of the tutorial. Finally we will discuss how can causality (or ideas from causality research) help DL and RL, focusing in particular on dealing with distribution shifts.</p>
                        <h4>Documents:</h4>
                        <div class="lecture-document-list">
                            No documents.
                        </div>
                        <h4>Lecture recordings:</h4>
                        <div class="lecture-document-list">
                            No recordings.
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="lecture-box" style="margin-bottom: 40px;">
                <button type="button" class="lecture-collapsible"><span class="fa fa-2x fa-university text-primary"></span><span class="lecture-title">Module 5: Diffusion and Advanced Generative Models</span></button>
                <div class="lecture-content">
                    <div class="padding-div">
                        <p class="text-muted" style="font-size: 12px"><b>Efstratios Gavves</b></p>
                        <p><b>Credits: 1pts </b><br></br> In this module we will study the interface and overlap between neural networks, dynamical systems, ordinary/partial/stochastic differential equations, and physics-based neural networks. We will study how and where dynamical systems be found in neural networks with implicit functions and neural ODEs. We will also see how neural networks can be used to model dynamical systems like Navier-Stokes with physics-informed neural networks, as well as with Fourier-inspired architectures and autoregressive neural networks.</p>
                        <h4>Documents:</h4>
                        <div class="lecture-document-list">
                            No documents.
                        </div>
                        <h4>Lecture recordings:</h4>
                        <div class="lecture-document-list">
                            No recordings.
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="lecture-box" style="">
                <button type="button" class="lecture-collapsible"><span class="fa fa-2x fa-university text-primary"></span><span class="lecture-title">Module 6: Neural Networks Dynamical Systems</span></button>
                <div class="lecture-content">
                    <div class="padding-div">
                        <p class="text-muted" style="font-size: 12px"><b>Efstratios Gavves</b></p>
                        <p><b>Credits: 2pts </b> <br></br> In this module we will study how one can integrate complex structure into neural networks. We will start with a quick introduction to Monte-Carlo simulation for neural networks. We will discuss Gumbel-Softmax  for differentiable models and the Gumbel-Argmax trick for sampling from categorical distributions, as well as then the Gumbel-Straight-Through variation. We will continue with subset sampling with Gumbel-Topk relaxations, sampling graphs in latent spaces in relational inference with GNN encoder/decode, as well as asampling latent permutations with Gumbel-Sinkhorn. Last, we will discuss about how discrete and continuous sampling connects to analysis of functions from mathematics.</p>
                        <h4>Documents:</h4>
                        <div class="lecture-document-list">
                            No documents.
                        </div>
                        <h4>Lecture recordings:</h4>
                        <div class="lecture-document-list">
                            No recordings.
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="lecture-box" style="margin-bottom: 40px;">
                <button type="button" class="lecture-collapsible"><span class="fa fa-2x fa-university text-primary"></span><span class="lecture-title">Module 7: Self-supervised and Vision-Language Learning</span></button>
                <div class="lecture-content">
                    <div class="padding-div">
                        <p class="text-muted" style="font-size: 12px"><b>Yuki Asano</b></p>
                        <p><b> Credits: 1pts </b><br></br> The recent trend of multi-modal learning from vision-language data demonstrates the abilities that can be unlocked by combining these modalities with ever increasing scale. Just in the last few months, models such as GPT-4, Flamingo, CLIP, FROMAGe have shown that in particular the combination with large language models (LLMs) is a key recipe for emerging capabilities of such models such as few-shot and in-context learning or zero-shot performances. In this module, we will focus on the intersection of self-supervised and multi-modal learning techniques that build the technical foundation for these recent achievements and methodologies. This then will allow us to understand and build on top of recent models that are available via APIs. <ul><li> <b>Lecture 1</b>: Refresh and extend the knowledge of self-supervised learning from DL1 to dive deeply into current approaches such as dense self-supervised learning and generative pretraining.</li><li> <b>Lecture 2:</b>: Explore the frontiers of vision-language methodologies, highlighting both the connection to self-supervised principles and the flexibility that comes from using language. We will specifically focus on the emergence and task of in-context learning.</li></ul></p>
                        <h4>Documents:</h4>
                        <div class="lecture-document-list">
                            No documents.
                        </div>
                        <h4>Lecture recordings:</h4>
                        <div class="lecture-document-list">
                            No recordings.
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="lecture-box" style="">
                <button type="button" class="lecture-collapsible"><span class="fa fa-2x fa-university text-primary"></span><span class="lecture-title">Module 8: Amortised Variational Inference</span></button>
                <div class="lecture-content">
                    <div class="padding-div">
                        <p class="text-muted" style="font-size: 12px"><b>Wilker Aziz Ferreira</b></p>
                        <p><b> Credits: 1pts </b><br></br> ....</p>
                        <h4>Documents:</h4>
                        <div class="lecture-document-list">
                            No documents.
                        </div>
                        <h4>Lecture recordings:</h4>
                        <div class="lecture-document-list">
                            No recordings.
                        </div>
                    </div>
                </div>
            </div>

        </div>
    </section>

    <div class="divider"></div>

    <section id="practicals">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Tutorials</h2>
                    <hr class="primary">
                </div>
            </div>

            <div class="lecture-box">
                <button type="button" class="lecture-collapsible"><span class="fa fa-2x fa-desktop text-primary"></span><span class="lecture-title">Module: Group equivariant deep learning</span></button>
                <div class="lecture-content">
                    <div class="padding-div">
                        <table><tr><td style="width: 100%; margin: auto;">
                            <p class="text-muted" style="font-size: 12px">Deadline: <!--$$DEADLINE$$--></p>
                            <p>This module covers the topic of geometric deep learning, touching upon all its five G's (Grids, Groups, Graphs, Geodesics, and Gauges) but with a strong focus on group equivariant deep learning. The impact that CNNs made in fields such as computer vision, computational chemistry and physics, can largely be attributed to the fact that convolutions allow for weight sharing, geometric stability, and a dramatic decrease in learnable parameters by leveraging symmetries in data and architecture design. These enabling properties arise from the equivariance property of convolutions. That is, if the input image is translated, the output of a convolution is translated accordingly, which in turn means that local information does not get lost in the neural network upon an input transformation (it is just shifted to a different location). With group equivariant deep learning we can hard-code stability and weight sharing over transformations beyond just translations. E.g., it allows for sharing of weights (representing complex patterns/representations) over poses and symmetries represented by transformations such as translation + rotation + scaling. This module is split into 4 lectures: <ul><li> <b>Lecture 1 (Wednesday 6th of April on-site Q&A)</b>: Regular group convolutional neural networks (G-CNNs). In this lecture we cover the basics of group convolutional NNs and show how to leverage symmetries in data and practical problems.</li>  <li> <b>Lecture 2 (Tuesday 12th of April on-site Q&A)</b>: Steerable G-CNNs. In this lecture we introduce a very general class of G-CNNs that allows to handle (rotational) symmetries in a flexible and powerful way. These methods are at the core of the most successful methods to handle 3D data such as atomic point clouds, but are also at the core of gauge equivariant methods that are applicable to arbitrary Riemannian manifolds (e.g. 2D shapes embedded in 3D space).</li> <li> <b>Lecture 3 (Tuesday 19th of April on-site Q&A)</b>: Equivariant graph NNs. Many problems in computational chemistry and computational physics are now-a-days solved via graph NNs. The SotA in these domains derive their effectives from the geometric structure and symmetries presented by the data and underlying physics. In this lecture we go over some of the state-of-the-art in geometric graph NNs, discussing several instances of the equivariant message passing paradigm. </li> <li> <b>Lecture 4 (Tuesday 26th of April on-site Q&A)</b>: Geometric latent space models. In this last lecture we take a step back from the group equivariance paradigm and look at what other types of geometric structure can be leveraged in neural networks. We will discuss some models that are based on non-Euclidean latent spaces and go over the required geometric tools to handle them.</li> </ul></p>
                            <h4>Tutorials:</h4>
                            <div class="lecture-document-list">
                                No documents.
                            </div>
                            <h4>Lecture recordings:</h4>
                            <div class="lecture-document-list">
                                No recordings.
                            </div>
                        </td><td>
                            <img src="<!--$$IMAGE$$-->" alt="" width="300" style="padding-bottom: 10px"></br>
                            <center class="text-muted" style="font-size: 12px"><!--$$IMAGE_DESC$$--></center>
                        </td></tr></table>
                    </div>
                </div>
            </div>
            
            <div class="lecture-box">
                <button type="button" class="lecture-collapsible"><span class="fa fa-2x fa-desktop text-primary"></span><span class="lecture-title">Module: Bayesian Deep Learning</span></button>
                <div class="lecture-content">
                    <div class="padding-div">
                        <table><tr><td style="width: 100%; margin: auto;">
                            <p class="text-muted" style="font-size: 12px">Deadline: <!--$$DEADLINE$$--></p>
                            <p>Usually we train neural networks (NNs) by way of point estimation.  After running gradient descent, each weight is represented by a particular value.  In this module, we consider giving NNs a Bayesian treatment.  We start by specifying a probability distribution for each weight---p(W)---and the goal of training is to obtain the posterior distribution p(W | D), where D denotes the training data, via Bayes rule.  The Bayesian approach provides natural mechanisms for model selection, complexity control, incorporating prior knowledge, uncertainty quantification, and online learning.  Yet, Bayesian inference is computationally demanding, and much of the module will focus on scalable methods for estimating the posterior distribution.</p>
                            <h4>Tutorials:</h4>
                            <div class="lecture-document-list">
                                No documents.
                            </div>
                            <h4>Lecture recordings:</h4>
                            <div class="lecture-document-list">
                                No recordings.
                            </div>
                        </td><td>
                            <img src="<!--$$IMAGE$$-->" alt="" width="300" style="padding-bottom: 10px"></br>
                            <center class="text-muted" style="font-size: 12px"><!--$$IMAGE_DESC$$--></center>
                        </td></tr></table>
                    </div>
                </div>
            </div>
            
            <div class="lecture-box">
                <button type="button" class="lecture-collapsible"><span class="fa fa-2x fa-desktop text-primary"></span><span class="lecture-title">Module: Deep probabilistic models I</span></button>
                <div class="lecture-content">
                    <div class="padding-div">
                        <table><tr><td style="width: 100%; margin: auto;">
                            <p class="text-muted" style="font-size: 12px">Deadline: <!--$$DEADLINE$$--></p>
                            <p>Add your questions for the Q&A: https://www.dory.app/c/f97ceccf/a8a8baab_dl2---probdl1 <br></br> 100% of the materials for this module are published. <br></br>In this module you learn to view data as a byproduct of probabilistic experiments. You will parameterise joint probability distributions over observed random variables and perform parameter estimation by regularised gradient-based maximum likelihood estimation. <br></br> The Q&A session for this module will take place at 6th of April.</p>
                            <h4>Tutorials:</h4>
                            <div class="lecture-document-list">
                                No documents.
                            </div>
                            <h4>Lecture recordings:</h4>
                            <div class="lecture-document-list">
                                No recordings.
                            </div>
                        </td><td>
                            <img src="<!--$$IMAGE$$-->" alt="" width="300" style="padding-bottom: 10px"></br>
                            <center class="text-muted" style="font-size: 12px"><!--$$IMAGE_DESC$$--></center>
                        </td></tr></table>
                    </div>
                </div>
            </div>
            
            <div class="lecture-box">
                <button type="button" class="lecture-collapsible"><span class="fa fa-2x fa-desktop text-primary"></span><span class="lecture-title">Module: Causality and causal representation learning</span></button>
                <div class="lecture-content">
                    <div class="padding-div">
                        <table><tr><td style="width: 100%; margin: auto;">
                            <p class="text-muted" style="font-size: 12px">Deadline: <!--$$DEADLINE$$--></p>
                            <p>This module will focus on the connections between causality and deep learning. We will start by introducing the basic concepts in causality (e.g. causal graphs, d-separations, interventions). Then we will focus on how can DL help causality, specifically for the task of causal discovery (learning the causal graph from data), which will be also the main topic of the tutorial. Finally we will discuss how can causality (or ideas from causality research) help DL and RL, focusing in particular on dealing with distribution shifts.</p>
                            <h4>Tutorials:</h4>
                            <div class="lecture-document-list">
                                No documents.
                            </div>
                            <h4>Lecture recordings:</h4>
                            <div class="lecture-document-list">
                                No recordings.
                            </div>
                        </td><td>
                            <img src="<!--$$IMAGE$$-->" alt="" width="300" style="padding-bottom: 10px"></br>
                            <center class="text-muted" style="font-size: 12px"><!--$$IMAGE_DESC$$--></center>
                        </td></tr></table>
                    </div>
                </div>
            </div>
            
            <div class="lecture-box">
                <button type="button" class="lecture-collapsible"><span class="fa fa-2x fa-desktop text-primary"></span><span class="lecture-title">Module: Diffusion and Advanced Generative Models</span></button>
                <div class="lecture-content">
                    <div class="padding-div">
                        <table><tr><td style="width: 100%; margin: auto;">
                            <p class="text-muted" style="font-size: 12px">Deadline: <!--$$DEADLINE$$--></p>
                            <p>In this module we will study the interface and overlap between neural networks, dynamical systems, ordinary/partial/stochastic differential equations, and physics-based neural networks. We will study how and where dynamical systems be found in neural networks with implicit functions and neural ODEs. We will also see how neural networks can be used to model dynamical systems like Navier-Stokes with physics-informed neural networks, as well as with Fourier-inspired architectures and autoregressive neural networks.</p>
                            <h4>Tutorials:</h4>
                            <div class="lecture-document-list">
                                No documents.
                            </div>
                            <h4>Lecture recordings:</h4>
                            <div class="lecture-document-list">
                                No recordings.
                            </div>
                        </td><td>
                            <img src="<!--$$IMAGE$$-->" alt="" width="300" style="padding-bottom: 10px"></br>
                            <center class="text-muted" style="font-size: 12px"><!--$$IMAGE_DESC$$--></center>
                        </td></tr></table>
                    </div>
                </div>
            </div>
            
            <div class="lecture-box">
                <button type="button" class="lecture-collapsible"><span class="fa fa-2x fa-desktop text-primary"></span><span class="lecture-title">Module: Neural Networks Dynamical Systems</span></button>
                <div class="lecture-content">
                    <div class="padding-div">
                        <table><tr><td style="width: 100%; margin: auto;">
                            <p class="text-muted" style="font-size: 12px">Deadline: <!--$$DEADLINE$$--></p>
                            <p>In this module we will study how one can integrate complex structure into neural networks. We will start with a quick introduction to Monte-Carlo simulation for neural networks. We will discuss Gumbel-Softmax  for differentiable models and the Gumbel-Argmax trick for sampling from categorical distributions, as well as then the Gumbel-Straight-Through variation. We will continue with subset sampling with Gumbel-Topk relaxations, sampling graphs in latent spaces in relational inference with GNN encoder/decode, as well as asampling latent permutations with Gumbel-Sinkhorn. Last, we will discuss about how discrete and continuous sampling connects to analysis of functions from mathematics.</p>
                            <h4>Tutorials:</h4>
                            <div class="lecture-document-list">
                                No documents.
                            </div>
                            <h4>Lecture recordings:</h4>
                            <div class="lecture-document-list">
                                No recordings.
                            </div>
                        </td><td>
                            <img src="<!--$$IMAGE$$-->" alt="" width="300" style="padding-bottom: 10px"></br>
                            <center class="text-muted" style="font-size: 12px"><!--$$IMAGE_DESC$$--></center>
                        </td></tr></table>
                    </div>
                </div>
            </div>
            
            <div class="lecture-box">
                <button type="button" class="lecture-collapsible"><span class="fa fa-2x fa-desktop text-primary"></span><span class="lecture-title">Module: Self-supervised and Vision-Language Learning</span></button>
                <div class="lecture-content">
                    <div class="padding-div">
                        <table><tr><td style="width: 100%; margin: auto;">
                            <p class="text-muted" style="font-size: 12px">Deadline: <!--$$DEADLINE$$--></p>
                            <p> Refresher on SSL principles and core techniques, deep dive into current directions (dense SSL, MAE), core works in combining vision and language under one umbrella.</p>
                            <h4>Tutorials:</h4>
                            <div class="lecture-document-list">
                                No documents.
                            </div>
                            <h4>Lecture recordings:</h4>
                            <div class="lecture-document-list">
                                No recordings.
                            </div>
                        </td><td>
                            <img src="<!--$$IMAGE$$-->" alt="" width="300" style="padding-bottom: 10px"></br>
                            <center class="text-muted" style="font-size: 12px"><!--$$IMAGE_DESC$$--></center>
                        </td></tr></table>
                    </div>
                </div>
            </div>

        </div>

    </section>


    <section id="course-links" class="bg-dark">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 text-center">
                    <h2 class="section-heading">Links</h2>
                    <hr class="primary">
                    <p style="text-align:justify; font-size:18px;">Some useful links for the course are the following:

                    <p>
                    <div align="left">

                    <ul>

                    <li> <a href="https://piazza.com/class/l136xghmlo16ai" target="_blank" style="font-size:16px;"> Piazza </a></li>
                    
                    <li> <a href="https://datanose.nl/#course[99881]" target="_blank" style="font-size:16px;"> Datanose </a></li>

                    <li> <a href="https://canvas.uva.nl/courses/28689" target="_blank" style="font-size:16px;"> Canvas </a></li>

                    <!-- <li> <a href="https://probabll.github.io/teaching/probdl/," target="_blank" style="font-size:16px;"> Deep probabilistic models </a></li> -->

                    <li> <a href="https://www.youtube.com/channel/UCxf_K59uNpwBs-O67MT50-A/playlists," target="_blank" style="font-size:16px;"> YouTube channel </a></li>
                    </ul>

                    </div>
                    </p>

                </div>
            </div>
        </div>
    </section>

    <section class="bg-dark" id="old-lectures">
        <div class="container text-center">
            <div class="call-to-action">
                <h2>If you are interested in older versions of the lectures, you can find them below.</h2>

                <!-- <a href="lectures-feb2016.html#" class="btn btn-default btn-xl wow" target="_blank">UVADLC Feb 2016</a> -->
                <!-- <a href="lectures-nov2016.html#" class="btn btn-default btn-xl wow" target="_blank">UVADLC Nov 2016</a> -->
                <a href="lectures-2022.html#" class="btn btn-default btn-xl wow" target="_blank">UVADLC Nov 2022</a>
            </div>
        </div>
    </section>

    <section id="contact">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 text-center">
                    <h2 class="section-heading">Contact us!</h2>
                    <hr class="primary">
                    <p class="running-text" style="text-align:justify">
                    If you have any questions or recommendations for the website or the course, you can always drop us a line!
                    The knowledge should be free, so feel also free to use any of the material provided here (but please be so kind to cite us).
                    In case you are a course instuctor and you want the solutions, please send us an email.
                    </p>
                </div>
                <div class="col-lg-4 col-lg-offset-2 text-center">
                    <i class="fa fa-map-marker fa-3x wow bounceIn"></i>
                    <p><a href="https://goo.gl/maps/HzB8CMdkTDA2">Room C3.229, Science Park 904 1098 XH, Amsterdam, The Netherlands</a></p>
                </div>
                <div class="col-lg-4 text-center">
                    <i class="fa fa-envelope-o fa-3x wow bounceIn" data-wow-delay=".1s"></i>
                    <p><a href="mailto:e.gavves@uva.nl">e.gavves@uva.nl</a></p>
                </div>
            </div>
        </div>
    </section>

    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="js/jquery.easing.min.js"></script>
    <script src="js/jquery.fittext.js"></script>
    <script src="js/wow.min.js"></script>

    <!-- Custom Theme JavaScript -->
    <script src="js/creative.js"></script>
    <script src="js/collapsible.js"></script>

</body>

</html>
