{"cells":[{"cell_type":"markdown","metadata":{"id":"8afrg8L7ACoE"},"source":["# Tutorial 1: Bayesian Neural Networks with Pyro\n","\n"]},{"cell_type":"markdown","source":["## Bayesian Neural Networks\n","\n","A Bayesian neural network is a probabilistic model that allows us to estimate uncertainty in predictions by representing the weights and biases of the network as probability distributions rather than fixed values. This allows us to *incorporate prior knowledge* about the weights and biases into the model, and *update our beliefs* about them as we observe data.\n","\n","Mathematically, a Bayesian neural network can be represented as follows:\n","\n","Given a set of input data $x$, we want to predict the corresponding output $y$. The neural network represents this relationship as a function $f(x, \\theta)$, where $\\theta$ are the weights and biases of the network. In a Bayesian neural network, we represent the weights and biases as probability distributions, so $f(x, \\theta)$ becomes a probability distribution over possible outputs:\n","\n","$$ p(y|x, \\mathcal{D}) = \\int p(y|x, \\theta)p(\\theta|\\mathcal{D}) d\\theta $$\n","\n","where $p(y|x, \\theta)$ is the likelihood function, which gives the probability of observing $y$ given $x$ and $\\theta$, and $p(\\theta|\\mathcal{D})$ is the posterior distribution over the weights and biases given the observed data $\\mathcal{D}$.\n","\n","To make predictions, we use the posterior predictive distribution:\n","\n","$$ p(y^*|x^*, \\mathcal{D}) = \\int p(y^*|x^*, \\theta)p(\\theta|\\mathcal{D}) d\\theta $$\n","\n","where $x^*$ is a new input and $y^*$ is the corresponding predicted output.\n","\n","To estimate the (intractable) posterior distribution $p(\\theta|\\mathcal{D})$, we can use either Markov Chain Monte Carlo (MCMC) or Variational Inference (VI)."],"metadata":{"id":"5wlbUXQ5mxts"}},{"cell_type":"markdown","source":["## Simulate data\n","Let's generate noisy observations from a sinusoidal function."],"metadata":{"id":"rtNNqRdIOmqK"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Set random seed for reproducibility\n","np.random.seed(42)\n","\n","# Generate data\n","x_obs = np.hstack([np.linspace(-0.2, 0.2, 500), np.linspace(0.6, 1, 500)])\n","noise = 0.02 * np.random.randn(x_obs.shape[0])\n","y_obs = x_obs + 0.3 * np.sin(2 * np.pi * (x_obs + noise)) + 0.3 * np.sin(4 * np.pi * (x_obs + noise)) + noise\n","\n","x_true = np.linspace(-0.5, 1.5, 1000)\n","y_true = x_true + 0.3 * np.sin(2 * np.pi * x_true) + 0.3 * np.sin(4 * np.pi * x_true)\n","\n","# Set plot limits and labels\n","xlims = [-0.5, 1.5]\n","ylims = [-1.5, 2.5]\n","\n","# Create plot\n","fig, ax = plt.subplots(figsize=(10, 5))\n","ax.plot(x_true, y_true, 'b-', linewidth=3, label=\"True function\")\n","ax.plot(x_obs, y_obs, 'ko', markersize=4, label=\"Observations\")\n","ax.set_xlim(xlims)\n","ax.set_ylim(ylims)\n","ax.set_xlabel(\"X\", fontsize=30)\n","ax.set_ylabel(\"Y\", fontsize=30)\n","ax.legend(loc=4, fontsize=15, frameon=False)\n","\n","plt.show()"],"metadata":{"id":"NIkB27gArnmm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r1VPB-FCdHzo"},"source":["## Getting started with Pyro"]},{"cell_type":"markdown","metadata":{"id":"0s1zQEID65Ta"},"source":["Let's install Pyro now.  You may have to restart the runtime after this step."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rh_UZO_UEyyw"},"outputs":[],"source":["!pip install pyro-ppl"]},{"cell_type":"markdown","metadata":{"id":"W7q2kdFwMJyY"},"source":["## Bayesian Neural Network with Gaussian Prior and Likelihood\n","Our first Bayesian neural network employs a Gaussian prior on the weights and a Gaussian likelihood function for the data. The network is a shallow neural network with one hidden layer.\n","\n","To be specific, we use the following prior on the weights $\\theta$:\n","\n","$p(\\theta) = \\mathcal{N}(\\mathbf{0}, 10\\cdot\\mathbb{I}),$ where $\\mathbb{I}$ is the identity matrix.\n","\n","To train the network, we define a likelihood function comparing the predicted outputs of the network with the actual data points:\n","\n","$p(y_i| x_i, \\theta) = \\mathcal{N}\\big(NN_{\\theta}(x_i), \\sigma^2\\big)$, with prior $\\sigma \\sim \\Gamma(1,1)$.\n","\n","Here, $y_i$ represents the actual output for the $i$-th data point, $x_i$ represents the input for that data point, $\\sigma$ is the standard deviation parameter for the normal distribution and $NN_{\\theta}$ is the shallow neural network parameterized by $\\theta$. \n","\n","Note that we use $\\sigma^2$ instead of $\\sigma$ in the likelihood function because we use a Gaussian prior on $\\sigma$ when performing variational inference and then want to avoid negative values for the standard deviation."]},{"cell_type":"code","source":["import pyro\n","import pyro.distributions as dist\n","from pyro.nn import PyroModule, PyroSample\n","import torch.nn as nn\n","\n","\n","class MyFirstBNN(PyroModule):\n","    def __init__(self, in_dim=1, out_dim=1, hid_dim=5, prior_scale=10.):\n","        super().__init__()\n","\n","        self.activation = nn.Tanh()  # or nn.ReLU()\n","        self.layer1 = PyroModule[nn.Linear](in_dim, hid_dim)  # Input to hidden layer\n","        self.layer2 = PyroModule[nn.Linear](hid_dim, out_dim)  # Hidden to output layer\n","\n","        # Set layer parameters as random variables\n","        self.layer1.weight = PyroSample(dist.Normal(0., prior_scale).expand([hid_dim, in_dim]).to_event(2))\n","        self.layer1.bias = PyroSample(dist.Normal(0., prior_scale).expand([hid_dim]).to_event(1))\n","        self.layer2.weight = PyroSample(dist.Normal(0., prior_scale).expand([out_dim, hid_dim]).to_event(2))\n","        self.layer2.bias = PyroSample(dist.Normal(0., prior_scale).expand([out_dim]).to_event(1))\n","\n","    def forward(self, x, y=None):\n","        x = x.reshape(-1, 1)\n","        x = self.activation(self.layer1(x))\n","        mu = self.layer2(x).squeeze()\n","        sigma = pyro.sample(\"sigma\", dist.Gamma(.5, 1))  # Infer the response noise\n","\n","        # Sampling model\n","        with pyro.plate(\"data\", x.shape[0]):\n","            obs = pyro.sample(\"obs\", dist.Normal(mu, sigma * sigma), obs=y)\n","        return mu"],"metadata":{"id":"CKnbkVoz_iFU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Define and run Markov chain Monte Carlo sampler\n","\n","To begin with, we can use MCMC to compute an *unbiased estimate* of $p(y|x, \\mathcal{D}) = \\mathbb{E}_{\\theta \\sim p(\\theta|\\mathcal{D})}\\big[p(y|x,\\theta)\\big]$ through Monte Carlo sampling. Specifically, we can approximate $\\mathbb{E}_{\\theta \\sim p(\\theta|\\mathcal{D})}\\big[p(y|x,\\theta)\\big]$ as follows:\n","$$\\mathbb{E}_{\\theta \\sim p(\\theta|\\mathcal{D})}\\big[p(y|x,\\theta)\\big] \\approx \\frac{1}{N} \\sum_{i=1}^{N} p(y|x,\\theta_{i}),$$\n","where $\\theta_{i} \\sim p(\\theta_i|\\mathcal{D}) \\propto p(\\mathcal{D}|\\theta)p(\\theta)$ are samples drawn from the posterior distribution. Because the normalizing constant is intractable, we require MCMC methods like Hamiltonian Monte Carlo to draw samples from the non-normalized posterior.\n","\n","Here, we use the No-U-Turn ([NUTS](https://arxiv.org/abs/1111.4246)) kernel."],"metadata":{"id":"7d14qstiA2Mq"}},{"cell_type":"code","source":["from pyro.infer import MCMC, NUTS\n","\n","model = MyFirstBNN()\n","\n","# Set Pyro random seed\n","pyro.set_rng_seed(42)\n","\n","# Define Hamiltonian Monte Carlo (HMC) kernel\n","# NUTS = \"No-U-Turn Sampler\" (https://arxiv.org/abs/1111.4246), gives HMC an adaptive step size\n","nuts_kernel = NUTS(model, jit_compile=False)  # jit_compile=True is faster but requires PyTorch 1.6+\n","\n","# Define MCMC sampler, get 50 posterior samples\n","mcmc = MCMC(nuts_kernel, num_samples=50)\n","\n","# Convert data to PyTorch tensors\n","x_train = torch.from_numpy(x_obs).float()\n","y_train = torch.from_numpy(y_obs).float()\n","\n","# Run MCMC\n","mcmc.run(x_train, y_train)"],"metadata":{"id":"UUeVvklgDDy-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BM-TzBWgM99Q"},"source":["We calculate and plot the predictive distribution."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SIG1pZkxansO"},"outputs":[],"source":["from pyro.infer import Predictive\n","\n","predictive = Predictive(model=model, posterior_samples=mcmc.get_samples())\n","x_test = torch.linspace(xlims[0], xlims[1], 3000)\n","preds = predictive(x_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yJRQ9WKfMqst"},"outputs":[],"source":["def plot_predictions(preds):\n","    y_pred = preds['obs'].T.detach().numpy().mean(axis=1)\n","    y_std = preds['obs'].T.detach().numpy().std(axis=1)\n","\n","    fig, ax = plt.subplots(figsize=(10, 5))\n","    xlims = [-0.5, 1.5]\n","    ylims = [-1.5, 2.5]\n","    plt.xlim(xlims)\n","    plt.ylim(ylims)\n","    plt.xlabel(\"X\", fontsize=30)\n","    plt.ylabel(\"Y\", fontsize=30)\n","\n","    ax.plot(x_true, y_true, 'b-', linewidth=3, label=\"true function\")\n","    ax.plot(x_obs, y_obs, 'ko', markersize=4, label=\"observations\")\n","    ax.plot(x_obs, y_obs, 'ko', markersize=3)\n","    ax.plot(x_test, y_pred, '-', linewidth=3, color=\"#408765\", label=\"predictive mean\")\n","    ax.fill_between(x_test, y_pred - 2 * y_std, y_pred + 2 * y_std, alpha=0.6, color='#86cfac', zorder=5)\n","\n","    plt.legend(loc=4, fontsize=15, frameon=False)"]},{"cell_type":"code","execution_count":null,"outputs":[],"source":["plot_predictions(preds)"],"metadata":{"id":"U-d9nN2-mAV7"}},{"cell_type":"markdown","metadata":{"id":"o7RjGbEy6_2K"},"source":["## Exercise 1: Deep Bayesian Neural Network\n","\n","We can define a deep Bayesian neural network in a similar fashion, with Gaussian priors on the weights: \n","\n","$p(\\theta) = \\mathcal{N}(\\mathbf{0}, 5\\cdot\\mathbb{I})$. \n","\n","The likelihood function is also Gaussian: \n","\n","$p(y_i| x_i, \\theta) = \\mathcal{N}\\big(NN_{\\theta}(x_i), \\sigma^2\\big)$, with $\\sigma \\sim \\Gamma(0.5,1)$.\n","\n","> Implement the deep Bayesian neural network and run MCMC to obtain posterior samples.\n","> Compute and plot the predictive distribution.\n","> Use the following network architecture: Number of hidden layers: 5, Number of hidden units per layer: 10, Activation function: Tanh, Prior scale: 5."]},{"cell_type":"code","source":["class BNN(PyroModule):\n","    def __init__(self, in_dim=1, out_dim=1, hid_dim=10, n_hid_layers=5, prior_scale=5.):\n","        super().__init__()\n","\n","        self.activation = nn.Tanh()  # could also be ReLU or LeakyReLU\n","        assert in_dim > 0 and out_dim > 0 and hid_dim > 0 and n_hid_layers > 0  # make sure the dimensions are valid\n","\n","        # TODO - define the layer sizes and the PyroModule layer list\n","        self.layers = NotImplemented\n","\n","        # TODO - define the prior distribution over the weights and biases\n","        for layer_idx, layer in enumerate(self.layers):\n","            layer.weight = NotImplemented\n","            layer.bias = NotImplemented\n","\n","    def forward(self, x, y=None):\n","        mu = NotImplemented  # TODO - compute the mean of the output distribution\n","        # TODO - sample the response noise\n","        # TODO - sample the response\n","        return mu"],"metadata":{"id":"pnjVP4wZGSNw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YQ1qofIiNSjM"},"source":["### Train the deep BNN with MCMC..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vbcKlUJd549g"},"outputs":[],"source":["# define model and data\n","model = BNN(hid_dim=10, n_hid_layers=5, prior_scale=5.)\n","\n","# define MCMC sampler\n","nuts_kernel = NUTS(model, jit_compile=False)\n","mcmc = MCMC(nuts_kernel, num_samples=50)\n","mcmc.run(x_train, y_train)"]},{"cell_type":"markdown","metadata":{"id":"5ZMsM58lNgEo"},"source":["Compute predictive distribution..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R0Xtrtqz7uel"},"outputs":[],"source":["predictive = Predictive(model=model, posterior_samples=mcmc.get_samples())\n","preds = predictive(x_test)\n","plot_predictions(preds)"]},{"cell_type":"markdown","source":["## Train BNNs with mean-field variational inference\n","\n","We will now move on to variational inference. Since the normalized posterior probability density $p(\\theta|\\mathcal{D})$ is intractable, we approximate it with a tractable parametrized density $q_{\\phi}(\\theta)$ in a family of probability densities $\\mathcal{Q}$. The variational parameters are denoted by $\\phi$ and the variational density is called the \"guide\" in Pyro. The goal is to find the variational probability density that best approximates the posterior by minimizing the KL divergence $$KL\\big(q_{\\phi}(\\theta)||p(\\theta|\\mathcal{D})\\big)$$ with respect to the variational parameters.\n","However, directly minimizing the KL divergence is not tractable because we assume that the posterior density is intractable. To solve this, we use Bayes theorem to obtain\n","$$\n","\\log p(\\mathcal{D}|\\theta) = KL\\big(q_{\\phi}(\\theta)||p(\\theta|\\mathcal{D})\\big) + ELBO(q_{\\phi}(\\theta)),\n","$$\n","where $ELBO(q_{\\phi}(\\theta))$ is the *Evidence Lower Bound*, given by\n","$$\n","ELBO(q_{\\phi}(\\theta)) = \\mathbb{E}_{\\theta \\sim q_{\\phi}(\\theta)}\\big[\\log p(y|x,\\theta) \\big] - KL\\big(q_{\\phi}(\\theta) || p(\\theta) \\big).\n","$$\n","By maximizing the ELBO, we indirectly minimize the KL divergence between the variational probability density and the posterior density."],"metadata":{"id":"xs1kQn7tHvO2"}},{"cell_type":"markdown","metadata":{"id":"0EeqtO-SN0kl"},"source":["Set up for stochastic variational inference with the variational density $q_{\\phi}(\\theta)$ by using a normal probability density with a diagonal covariance matrix:"]},{"cell_type":"code","source":["from pyro.infer import SVI, Trace_ELBO\n","from pyro.infer.autoguide import AutoDiagonalNormal\n","from tqdm.auto import trange\n","pyro.clear_param_store()\n","\n","model = BNN(hid_dim=10, n_hid_layers=5, prior_scale=5.)\n","mean_field_guide = AutoDiagonalNormal(model)\n","optimizer = pyro.optim.Adam({\"lr\": 0.01})\n","\n","svi = SVI(model, mean_field_guide, optimizer, loss=Trace_ELBO())\n","pyro.clear_param_store()\n","\n","num_epochs = 25000\n","progress_bar = trange(num_epochs)\n","\n","for epoch in progress_bar:\n","    loss = svi.step(x_train, y_train)\n","    progress_bar.set_postfix(loss=f\"{loss / x_train.shape[0]:.3f}\")"],"metadata":{"id":"2bFbuA5fJmFl","pycharm":{"is_executing":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vefi5wv-OFfI"},"source":["As before, we compute the predictive distribution sampling from the trained variational density."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R28wesFt3DjZ","pycharm":{"is_executing":true}},"outputs":[],"source":["predictive = Predictive(model, guide=mean_field_guide, num_samples=500)\n","preds = predictive(x_test)\n","plot_predictions(preds)"]},{"cell_type":"markdown","source":["## Exercise 2: Bayesian updating with variational inference"],"metadata":{"id":"APlZryo9nxBg"}},{"cell_type":"markdown","metadata":{"id":"IY8G3rLrR2mK"},"source":["What happens if we obtain new data points, denoted as $\\mathcal{D}'$, after performing variational inference using the observations $\\mathcal{D}$?"]},{"cell_type":"code","source":["# Generate new observations\n","x_new = np.linspace(0.2, 0.6, 100)\n","noise = 0.02 * np.random.randn(x_new.shape[0])\n","y_new = x_new + 0.3 * np.sin(2 * np.pi * (x_new + noise)) + 0.3 * np.sin(4 * np.pi * (x_new + noise)) + noise\n","\n","# Generate true function\n","x_true = np.linspace(-0.5, 1.5, 1000)\n","y_true = x_true + 0.3 * np.sin(2 * np.pi * x_true) + 0.3 * np.sin(4 * np.pi * x_true)\n","\n","# Set axis limits and labels\n","plt.xlim(xlims)\n","plt.ylim(ylims)\n","plt.xlabel(\"X\", fontsize=30)\n","plt.ylabel(\"Y\", fontsize=30)\n","\n","# Plot all datasets\n","plt.plot(x_true, y_true, 'b-', linewidth=3, label=\"True function\")\n","plt.plot(x_new, y_new, 'ko', markersize=4, label=\"New observations\", c=\"r\")\n","plt.plot(x_obs, y_obs, 'ko', markersize=4, label=\"Old observations\")\n","plt.legend(loc=4, fontsize=15, frameon=False)\n","plt.show()"],"metadata":{"id":"vZ-qsDisMUSq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Bayesian update\n","\n","How can we perform a Bayesian update on the model using variational inference when new observations become available?\n","\n","We can use the previously calculated posterior probability density as the new prior and update the posterior with the new observations. Specifically, the updated posterior probability density is given by:\n","\n","$$\n","p(\\theta|\\mathcal{D}') = \\frac{p(\\mathcal{D}'|\\theta)q_{\\phi}(\\theta)}{\\int p(\\mathcal{D}'|\\theta)q_{\\phi}(\\theta)}\n","$$\n","\n","Note that we want to update our model using only the new observations $\\mathcal{D}'$, relying on the fact that the variational density used as our new prior carries the necessary information on the old observations $\\mathcal{D}$.\n","\n","\n","### Implementation in Pyro\n","\n","To implement this in Pyro, we can extract the variational parameters (mean and standard deviation) from the `guide` and use them to initialize the prior in a new model that is similar to the original model used for variational inference.\n","\n","From the Gaussian `guide` we can extract the variational parameters (mean and standard deviation) as:\n","\n","```python\n","mu = guide.get_posterior().mean\n","sigma = guide.get_posterior().stddev\n","```\n","\n","\n","\n"],"metadata":{"id":"r7_9erXVMy6i"}},{"cell_type":"markdown","source":["### Exercise 2.1 Learn a model on the old observations\n","First, as before, we define a model using Gaussian prior $\\mathcal{N}(\\mathbf{0}, 10\\cdot \\mathbb{I})$.\n","\n","> Train a model `MyFirstBNN` on the old observations $\\mathcal{D}$ using variational inference with `AutoDiagonalNormal()` as guide."],"metadata":{"collapsed":false,"id":"w6jq_L1QmAV8"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["from pyro.optim import Adam\n","pyro.set_rng_seed(42)\n","pyro.clear_param_store()\n","\n","model = MyFirstBNN()\n","guide = NotImplemented  # TODO: Use AutoDiagonalNormal\n","# TODO: Learn the model using stochastic variational inference"],"metadata":{"id":"Bq4X6c3OmAV8"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["predictive = Predictive(model, guide=guide, num_samples=5000)\n","preds = predictive(x_test)\n","plot_predictions(preds)"],"metadata":{"id":"Ojrx15hrmAV8"}},{"cell_type":"markdown","source":["Next, we can extract the variational parameters (mean and standard deviation) from the `guide` and use them to initialize the prior in a new model that is similar to the original model used for variational inference."],"metadata":{"collapsed":false,"id":"3qq_KfKTmAV8"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["# Extract variational parameters from guide\n","mu = guide.get_posterior().mean.detach()\n","stddev = guide.get_posterior().stddev.detach()"],"metadata":{"id":"vcgM6OGlmAV8"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["for name, value in pyro.get_param_store().items():\n","    print(name, pyro.param(name))"],"metadata":{"id":"NaXAlZVLmAV8"}},{"cell_type":"markdown","source":["### Exercise 2.2 Initialize a second model with the variational parameters\n","> Define a new model similar to `MyFirstBNN(PyroModule)`, that takes the variational parameters and uses them to initialize the prior."],"metadata":{"id":"oocfregiR04Y"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["class UpdatedBNN(PyroModule):\n","    def __init__(self, mu, stddev, in_dim=1, out_dim=1, hid_dim=5):\n","        super().__init__()\n","        self.mu = mu\n","        self.stddev = stddev\n","\n","        self.activation = nn.Tanh()\n","        self.layer1 = PyroModule[nn.Linear](in_dim, hid_dim)\n","        self.layer2 = PyroModule[nn.Linear](hid_dim, out_dim)\n","\n","        self.layer1.weight = NotImplemented  # TODO: Initialize from mu and stddev\n","        self.layer1.bias =  NotImplemented  # TODO: Initialize from mu and stddev\n","        self.layer2.weight = NotImplemented  # TODO: Initialize from mu and stddev\n","        self.layer2.bias = NotImplemented  # TODO: Initialize from mu and stddev\n","        # 17th parameter is parameter sigma from the Gamma distribution\n","\n","    def forward(self, x, y=None):\n","        x = x.reshape(-1, 1)\n","        x = self.activation(self.layer1(x))\n","        mu = self.layer2(x).squeeze()\n","        sigma = pyro.sample(\"sigma\", dist.Gamma(.5, 1))\n","\n","        with pyro.plate(\"data\", x.shape[0]):\n","            obs = pyro.sample(\"obs\", dist.Normal(mu, sigma * sigma), obs=y)\n","        return mu"],"metadata":{"id":"TYT5Dk7VmAV8"}},{"cell_type":"markdown","source":["### Exercise 2.3 Perform variational inference on the new model\n","> Then perform variational inference on this new model using the new observations and plot the predictive distribution.\n","> What do you observe? How does the predictive distribution compare to the one obtained in Exercise 2.1?"],"metadata":{"collapsed":false,"id":"-DFanFGxmAV9"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["x_train_new = torch.from_numpy(x_new).float()\n","y_train_new = torch.from_numpy(y_new).float()\n","\n","pyro.clear_param_store()\n","\n","# TODO: Initialize the new model with the variational parameters\n","new_model = NotImplemented\n","new_guide = NotImplemented\n","\n","# TODO: Perform variational inference on the new model"],"metadata":{"id":"u3uwI3AOmAV9"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["predictive = Predictive(new_model, guide=new_guide, num_samples=5000)\n","preds = predictive(x_test)\n","plot_predictions(preds)"],"metadata":{"id":"olgiLeo1mAV9"}}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"}},"nbformat":4,"nbformat_minor":0}