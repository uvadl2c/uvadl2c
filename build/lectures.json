[
	{
		"type": "Lecture",
		"name": "Geometric Deep Learning",
		"date": "Erik Bekkers",
		"desc": "This module covers the topic of geometric deep learning [1], touching upon all its five Gâ€™s (Grids, Groups, Graphs, Geodesics, and Gauges) but with a strong focus on group equivariant deep learning. The impact that CNNs made in fields such as computer vision, computational chemistry and physics, can largely be attributed to the fact that convolutions allow for weight sharing, geometric stability, and a dramatic decrease in learnable parameters by leveraging symmetries in data and architecture design. These enabling properties arise from the equivariance property of convolutions. That is, if the input image is translated, the output of a convolution is translated accordingly, which in turn means that local information does not get lost in the neural network upon an input transformation (it is just shifted to a different location). With group equivariant deep learning we can hard-code stability and weight sharing over transformations beyond just translations. E.g., it allows for sharing of weights (representing complex patterns/representations) over poses and symmetries represented by transformations such as translation + rotation + scaling.",
		"documents": [

			{"name": "Group equivariant DL (regular g-convs).",
				"link": "",
				"type": ""},
			{"name": "Group Equivariant DL (steerable g-convs).",
				"link": "",
				"type": ""},
			{"name": "Equivariant graph neural networks.",
				"link": "",
				"type": ""}

		],
		"recordings": [
		]
	},

	{
		"type": "Lecture",
		"name": "Bayesian Deep Learning",
		"date": "Eric Nalisnick",
		"desc": "...",
		"documents": [
			{"name": "Bayes NN basics: motivation, priors, posterior inference (VI---Bayes by backprop, moment prop---and MCMC---scalable HMC), infinite limits",
				"link": "",
				"type": ""}
		],
		"recordings": [
		]
	},

	{
		"type": "Lecture",
		"name": "Deep probabilistic models I",
		"date": "Wilker Aziz Ferreira",
		"desc": "...",
		"documents": [
			{"name": "NN parameterisation of joint distributions over observed variables",
				"link": "",
				"type": ""},
			{"name": "Tractable deep latent variable models",
				"link": "",
				"type": ""}
	],
		"recordings": [
		]
	},

	

	{
		"type": "Module",
		"name": "Bayesian Deep Learning",
		"date": "Eric Nalisnick",
		"desc": "...",
		"documents": [
			{"name": "",
				"link": "",
				"type": ""}
		],
		"recordings": [
		]
	},

	{
		"type": "Module",
		"name": "Deep probabilistic models II",
		"date": "Wilker Aziz Ferreira",
		"desc": "...",
		"documents": [

			{"name": "",
				"link": "",
				"type": ""}
		],
		"recordings": [
		]
	},

	{
		"type": "Module",
		"name": "Advanced approximate inference for discrete latent variable models",
		"date": "Wilker Aziz Ferreira",
		"desc": "...",
		"documents": [

			{"name": "",
				"link": "",
				"type": ""}
		],
		"recordings": [
		]
	},


	{
		"type": "Module",
		"name": "Causality and Deep Learning",
		"date": "Sara Magliacane",
		"desc": "...",
		"documents": [
			{"name": "",
				"link": "",
				"type": ""}
		],
		"recordings": [
		]
	},

	{
		"type": "Module",
		"name": "High-performant Deep Learning",
		"date": "Jonas Teuwen",
		"desc": "...",
		"documents": [

			{"name": "",
				"link": "",
				"type": ""}
		],
		"recordings": [
		]
	},


	{
		"type": "Module",
		"name": "Advanced Generative Models",
		"date": "Efstratios Gavves+Emiel Hogenboom",
		"desc": "...",
		"documents": [
			{"name": "",
				"link": "",
				"type": ""}
		],
		"recordings": [
			
		]
	},

	{
		"type": "Module",
		"name": "Neural Network Dynamical Systems",
		"date": "Efstratios Gavves",
		"desc": "...",
		"documents": [
		],
		"recordings": [
		]
	},


	{
		"type": "Module",
		"name": "Sampling & Gradient Approximations",
		"date": "Efstratios Gavves",
		"desc": "...",
		"documents": [
		],
		"recordings": [
		]
	}

]
