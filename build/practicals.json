[
	{
		"name": "Group equivariant deep learning",
		"desc": "This module covers the topic of geometric deep learning, touching upon all its five G's (Grids, Groups, Graphs, Geodesics, and Gauges) but with a strong focus on group equivariant deep learning. The impact that CNNs made in fields such as computer vision, computational chemistry and physics, can largely be attributed to the fact that convolutions allow for weight sharing, geometric stability, and a dramatic decrease in learnable parameters by leveraging symmetries in data and architecture design. These enabling properties arise from the equivariance property of convolutions. In this module you will learn how to equip neural networks with equivariance properties.",
		"documents": [
		
			{"name": "GDL - Regular Group Convolutions",
				 "link": "https://uvadlc-notebooks.readthedocs.io/en/latest/",
				 "type": "ipynb"},
			{"name": "GDL - Steerable CNNs",
				 "link": "https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/Geometric_deep_learning/tutorial2_steerable_cnns.html",
				 "type": "ipynb"}

				 
			],
			"recordings": [
			{"name": "GDL - Regular Group Convolutions",
				 "link": "https://www.youtube.com/watch?v=7bZB4HtZlk4&t=2764s&ab_channel=DeepLearningII",
				 "type": "video"},
			{"name": "GDL - Steerable CNNs",
				 "link": "https://amsuni-my.sharepoint.com/personal/g_cesa_uva_nl/_layouts/15/stream.aspx?id=%2Fpersonal%2Fg%5Fcesa%5Fuva%5Fnl%2FDocuments%2Fgdl%5Ftutorial2%2Emp4&ga=1",
				 "type": "video"}
	
			]
	},

	{
		"name": "Bayesian Deep Learning",
		"desc": "Usually we train neural networks (NNs) by way of point estimation.  After running gradient descent, each weight is represented by a particular value.  In this module, we consider giving NNs a Bayesian treatment.  We start by specifying a probability distribution for each weight---p(W)---and the goal of training is to obtain the posterior distribution p(W | D), where D denotes the training data, via Bayes rule.  The Bayesian approach provides natural mechanisms for model selection, complexity control, incorporating prior knowledge, uncertainty quantification, and online learning.  Yet, Bayesian inference is computationally demanding, and much of the module will focus on scalable methods for estimating the posterior distribution.",
		"documents": [
		
			{"name": "Bayesian Neural Networks with Pyro",
				 "link": "lectures/Bayesian Deep Learning/dl2_bnn_tut1_students.ipynb",
				 "type": "ipynb"},
				 {"name": "Comparing to Non-Bayesian Methods for Uncertainty",
				 "link": "https://colab.research.google.com/drive/1n2GAT_oE4kSi2BfJMTMG7yeLsHYoikmJ?usp=share_link",
				 "type": "ipynb"}
				 
			],
			"recordings": [
			{"name": "TBA",
				 "link": "...",
				 "type": "video"}
	
			]
	},

	{
		"name": "Deep probabilistic models",
		"desc": "Many (if not most) advanced DL models are probabilistic models (or at the very least key aspects of their design and training are given probabilistic treatment). The focus of this module (or this part of the module) is to learn to prescribe probability distributions over complex sample spaces (discrete, continuous, structured), parameterise these distributions using NNs, and estimate model parameters to maximise (bounds on) likelihood via gradient descent. The goal is to get students to expand their toolbox, to see modelling ideas and estimation algorithms as modules they can compose (ie, VI is not exclusive to VAEs, VAEs are not necessarily built upon Gaussians, autoregressive models are not exclusive to one data type or another, reparameterisation is a general tool, MLE is a general tool, etc). We cover two main classes of models, depending on whether a key function (the likelihood function) can be assessed tractably given a set of observations and a parameter vector. <br></br> <b>TL;DR</b> In this module you learn to view data as a byproduct of probabilistic experiments. You will parameterise joint probability distributions over observed random variables, however complex/structured they may be, and perform parameter estimation by regularised gradient-based maximum likelihood estimation. </br> <br><b>Relationship to other modules:</b></br> <ul><li>Advanced generative models are (rather special) instances of probabilistic models, this module gives you some background knowledge that can ease your way into advanced generative models such as normalising flows, energy-based models and diffusion processes.</li> <li>Certain advanced probabilistic models (e.g., latent variable models) require techniques to approximate intractable computations in a principled manner, those techniques are discussed in the amortised variational inference module. Because amortised VI concerns probabilistic models, this module can be thought of as background to it.</li><li>Bayesian models are also probabilistic, but you don't necessarily need to content of this module to understand Bayesian deep learning (it does help, but you can live without).</li></ul>",
		"documents": [

	],
		"recordings": [

		]
	},


	{
		"name": "Causality and causal representation learning",
		"desc": "This module will focus on the connections between causality and deep learning. We will start by introducing the basic concepts in causality (e.g. causal graphs, d-separations, interventions). Then we will focus on how can DL help causality, specifically for the task of causal discovery (learning the causal graph from data), which will be also the main topic of the tutorial. Finally we will discuss how can causality (or ideas from causality research) help DL and RL, focusing in particular on dealing with distribution shifts.",
		"documents": [

		],
		"recordings": [

		]
	},


	{
		"name": "Deep Generative Models",
		"desc": "In this module we will study how one can integrate complex structure into neural networks. We will start with a quick introduction to Monte-Carlo simulation for neural networks. We will discuss Gumbel-Softmax  for differentiable models and the Gumbel-Argmax trick for sampling from categorical distributions, as well as then the Gumbel-Straight-Through variation. We will continue with subset sampling with Gumbel-Topk relaxations, sampling graphs in latent spaces in relational inference with GNN encoder/decode, as well as asampling latent permutations with Gumbel-Sinkhorn. Last, we will discuss about how discrete and continuous sampling connects to analysis of functions from mathematics.",
		"documents": [


		],
		"recordings": [

		]
	},


	{
		"name": "Neural Networks Dynamical Systems",
		"desc": "In this module we will study the interface and overlap between neural networks, dynamical systems, ordinary/partial/stochastic differential equations, and physics-based neural networks. We will study how and where dynamical systems be found in neural networks with implicit functions and neural ODEs. We will also see how neural networks can be used to model dynamical systems like Navier-Stokes with physics-informed neural networks, as well as with Fourier-inspired architectures and autoregressive neural networks.",
		"documents": [


		],
		"recordings": [
		]
	},

	{
		"name": "Self-supervised and Vision-Language Learning",
		"desc": " Refresher on SSL principles and core techniques, deep dive into current directions (dense SSL, MAE), core works in combining vision and language under one umbrella.",
		"documents": [

		],
		"recordings": [

		]
	}

]